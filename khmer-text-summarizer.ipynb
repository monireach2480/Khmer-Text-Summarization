{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6fd6190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ khmernltk loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from math import sqrt, log\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# NLTK imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Sklearn for TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Try importing Khmer NLP with fallback\n",
    "try:\n",
    "    from khmernltk import sentence_tokenize, word_tokenize as khmer_word_tokenize\n",
    "    KHMERNLTK_AVAILABLE = True\n",
    "    print(\"✓ khmernltk loaded successfully\")\n",
    "except ImportError:\n",
    "    KHMERNLTK_AVAILABLE = False\n",
    "    print(\"⚠ Warning: khmernltk not installed. Using fallback tokenization.\")\n",
    "    print(\"  Install with: pip install khmernltk or Install with: pip install khmer-nltk\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0939e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KhmerTextPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing pipeline for Khmer documents\n",
    "    Uses NLTK utilities and Khmer-specific tokenization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stopwords_file=\"khmer_stopwords.txt\"):\n",
    "        \"\"\"Initialize preprocessor with stopwords\"\"\"\n",
    "        self.stopwords = self.load_stopwords(stopwords_file)\n",
    "        print(f\"Loaded {len(self.stopwords)} Khmer stopwords\")\n",
    "    \n",
    "    def load_stopwords(self, file_path: str) -> set:\n",
    "        \"\"\"Load Khmer stopwords from file\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    stopwords = set(line.strip() for line in file if line.strip())\n",
    "                    return stopwords\n",
    "            else:\n",
    "                print(f\"Warning: {file_path} not found. Using default stopwords.\")\n",
    "                # Default common Khmer stopwords\n",
    "                default_stopwords = {\n",
    "                    \"នេះ\", \"នោះ\", \"ទាំង\", \"ដែល\", \"ជា\", \"មាន\", \"និង\", \"បាន\", \n",
    "                    \"ក្នុង\", \"ពី\", \"គឺ\", \"ដោយ\", \"ទៅ\", \"ឬ\", \"ផង\", \"ទេ\"\n",
    "                }\n",
    "                return default_stopwords\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading stopwords: {e}\")\n",
    "            return set()\n",
    "    \n",
    "    def normalize_khmer_text(self, text: str) -> str:\n",
    "        \"\"\"Normalize Khmer text - handle Unicode variations and clean text\"\"\"\n",
    "        # Remove extra whitespaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Normalize Khmer punctuation\n",
    "        text = re.sub(r'[។]+', '។', text)  # Multiple periods\n",
    "        text = re.sub(r'[៕]+', '៕', text)  # Multiple section marks\n",
    "        \n",
    "        # Remove zero-width characters\n",
    "        text = text.replace('\\u200b', '')  # Zero-width space\n",
    "        text = text.replace('\\u200c', '')  # Zero-width non-joiner\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def remove_punctuation(self, text: str) -> str:\n",
    "        \"\"\"Remove Khmer and English punctuation\"\"\"\n",
    "        # Khmer punctuation\n",
    "        khmer_punct = \"។៕៖៙៚៛៝៞៟\"\n",
    "        # English punctuation\n",
    "        english_punct = \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
    "        \n",
    "        for punct in khmer_punct + english_punct:\n",
    "            text = text.replace(punct, \" \")\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize text into sentences\"\"\"\n",
    "        if KHMERNLTK_AVAILABLE:\n",
    "            try:\n",
    "                sentences = sentence_tokenize(text)\n",
    "                sentences = [s.strip() for s in sentences if len(s.strip()) > 10]\n",
    "                return sentences\n",
    "            except Exception as e:\n",
    "                print(f\"Sentence tokenization error: {e}. Using fallback.\")\n",
    "        \n",
    "        # Fallback: split by Khmer period\n",
    "        sentences = text.split('។')\n",
    "        sentences = [s.strip() + '។' for s in sentences if len(s.strip()) > 10]\n",
    "        return sentences\n",
    "    \n",
    "    def tokenize_words(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize text into words\"\"\"\n",
    "        if KHMERNLTK_AVAILABLE:\n",
    "            try:\n",
    "                words = khmer_word_tokenize(text)\n",
    "                words = [w.strip() for w in words if len(w.strip()) > 0]\n",
    "                return words\n",
    "            except Exception as e:\n",
    "                print(f\"Word tokenization error: {e}. Using fallback.\")\n",
    "        \n",
    "        # Fallback: simple space split\n",
    "        return text.split()\n",
    "    \n",
    "    def remove_stopwords(self, words: List[str]) -> List[str]:\n",
    "        \"\"\"Remove stopwords from word list\"\"\"\n",
    "        return [word for word in words if word.lower() not in self.stopwords]\n",
    "    \n",
    "    def preprocess_text(self, text: str, remove_punct: bool = True, \n",
    "                       remove_stops: bool = True, normalize: bool = True) -> List[str]:\n",
    "        \"\"\"Complete preprocessing pipeline for text\"\"\"\n",
    "        if normalize:\n",
    "            text = self.normalize_khmer_text(text)\n",
    "        \n",
    "        if remove_punct:\n",
    "            text = self.remove_punctuation(text)\n",
    "        \n",
    "        words = self.tokenize_words(text)\n",
    "        words = [word.lower() for word in words]\n",
    "        \n",
    "        if remove_stops:\n",
    "            words = self.remove_stopwords(words)\n",
    "        \n",
    "        words = [word for word in words if len(word) > 1]\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    def preprocess_sentences(self, text: str) -> Tuple[List[str], List[List[str]]]:\n",
    "        \"\"\"Preprocess document maintaining sentence structure\"\"\"\n",
    "        original_sentences = self.tokenize_sentences(text)\n",
    "        \n",
    "        processed_sentences = []\n",
    "        for sentence in original_sentences:\n",
    "            words = self.preprocess_text(sentence, remove_punct=True, \n",
    "                                        remove_stops=True, normalize=True)\n",
    "            if words:\n",
    "                processed_sentences.append(words)\n",
    "        \n",
    "        return original_sentences[:len(processed_sentences)], processed_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fa15263",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRankSummarizer:\n",
    "    \"\"\"TextRank algorithm for extractive summarization\"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor: KhmerTextPreprocessor):\n",
    "        self.preprocessor = preprocessor\n",
    "    \n",
    "    def cosine_similarity_vectors(self, vec1: List[float], vec2: List[float]) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "        dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "        norm1 = sqrt(sum(a * a for a in vec1))\n",
    "        norm2 = sqrt(sum(b * b for b in vec2))\n",
    "        \n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return dot_product / (norm1 * norm2)\n",
    "    \n",
    "    def sentence_similarity(self, sent1: List[str], sent2: List[str]) -> float:\n",
    "        \"\"\"Calculate similarity between two sentences\"\"\"\n",
    "        all_words = list(set(sent1 + sent2))\n",
    "        \n",
    "        if not all_words:\n",
    "            return 0.0\n",
    "        \n",
    "        vector1 = [sent1.count(word) for word in all_words]\n",
    "        vector2 = [sent2.count(word) for word in all_words]\n",
    "        \n",
    "        return self.cosine_similarity_vectors(vector1, vector2)\n",
    "    \n",
    "    def build_similarity_matrix(self, sentences: List[List[str]]) -> np.ndarray:\n",
    "        \"\"\"Build sentence similarity matrix for graph construction\"\"\"\n",
    "        n = len(sentences)\n",
    "        similarity_matrix = np.zeros((n, n))\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    similarity_matrix[i][j] = self.sentence_similarity(\n",
    "                        sentences[i], sentences[j]\n",
    "                    )\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def summarize(self, text: str, num_sentences: int = 3, \n",
    "                 summary_ratio: float = None) -> Dict:\n",
    "        \"\"\"Generate extractive summary using TextRank\"\"\"\n",
    "        original_sentences, processed_sentences = self.preprocessor.preprocess_sentences(text)\n",
    "        \n",
    "        if len(original_sentences) == 0:\n",
    "            return {\n",
    "                'summary': \"\",\n",
    "                'method': 'TextRank',\n",
    "                'num_sentences': 0,\n",
    "                'error': 'No sentences found'\n",
    "            }\n",
    "        \n",
    "        if summary_ratio:\n",
    "            num_sentences = max(1, int(len(original_sentences) * summary_ratio))\n",
    "        else:\n",
    "            num_sentences = min(num_sentences, len(original_sentences))\n",
    "        \n",
    "        if len(original_sentences) <= num_sentences:\n",
    "            summary_text = '។ '.join(original_sentences)\n",
    "            return {\n",
    "                'summary': summary_text,\n",
    "                'method': 'TextRank',\n",
    "                'num_sentences': len(original_sentences),\n",
    "                'note': 'Document too short, returned original'\n",
    "            }\n",
    "        \n",
    "        similarity_matrix = self.build_similarity_matrix(processed_sentences)\n",
    "        similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "        scores = nx.pagerank(similarity_graph, max_iter=100)\n",
    "        \n",
    "        ranked_sentences = sorted(\n",
    "            [(scores[i], i, original_sentences[i]) for i in range(len(original_sentences))],\n",
    "            reverse=True,\n",
    "            key=lambda x: x[0]\n",
    "        )\n",
    "        \n",
    "        selected_indices = sorted([sent[1] for sent in ranked_sentences[:num_sentences]])\n",
    "        summary_sentences = [original_sentences[i] for i in selected_indices]\n",
    "        \n",
    "        summary_text = '។ '.join(summary_sentences)\n",
    "        summary_text = summary_text.replace('។។', '។')\n",
    "        \n",
    "        return {\n",
    "            'summary': summary_text,\n",
    "            'method': 'TextRank',\n",
    "            'num_sentences': num_sentences,\n",
    "            'total_sentences': len(original_sentences),\n",
    "            'compression_ratio': num_sentences / len(original_sentences),\n",
    "            'sentence_scores': {i: scores[i] for i in range(len(original_sentences))}\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b98baeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFSummarizer:\n",
    "    \"\"\"TF-IDF based extractive summarization\"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor: KhmerTextPreprocessor):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.vectorizer = None\n",
    "    \n",
    "    def summarize(self, text: str, num_sentences: int = 3, \n",
    "                 summary_ratio: float = None) -> Dict:\n",
    "        \"\"\"Generate summary using TF-IDF sentence scoring\"\"\"\n",
    "        original_sentences, processed_sentences = self.preprocessor.preprocess_sentences(text)\n",
    "        \n",
    "        if len(original_sentences) == 0:\n",
    "            return {\n",
    "                'summary': \"\",\n",
    "                'method': 'TF-IDF',\n",
    "                'num_sentences': 0,\n",
    "                'error': 'No sentences found'\n",
    "            }\n",
    "        \n",
    "        if summary_ratio:\n",
    "            num_sentences = max(1, int(len(original_sentences) * summary_ratio))\n",
    "        else:\n",
    "            num_sentences = min(num_sentences, len(original_sentences))\n",
    "        \n",
    "        if len(original_sentences) <= num_sentences:\n",
    "            summary_text = '។ '.join(original_sentences)\n",
    "            return {\n",
    "                'summary': summary_text,\n",
    "                'method': 'TF-IDF',\n",
    "                'num_sentences': len(original_sentences),\n",
    "                'note': 'Document too short'\n",
    "            }\n",
    "        \n",
    "        sentence_strings = [' '.join(sent) for sent in processed_sentences]\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(sentence_strings)\n",
    "        \n",
    "        sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
    "        \n",
    "        ranked_indices = np.argsort(sentence_scores)[::-1]\n",
    "        \n",
    "        selected_indices = sorted(ranked_indices[:num_sentences])\n",
    "        summary_sentences = [original_sentences[i] for i in selected_indices]\n",
    "        \n",
    "        summary_text = '។ '.join(summary_sentences)\n",
    "        summary_text = summary_text.replace('។។', '។')\n",
    "        \n",
    "        return {\n",
    "            'summary': summary_text,\n",
    "            'method': 'TF-IDF',\n",
    "            'num_sentences': num_sentences,\n",
    "            'total_sentences': len(original_sentences),\n",
    "            'compression_ratio': num_sentences / len(original_sentences),\n",
    "            'sentence_scores': {i: float(sentence_scores[i]) for i in range(len(sentence_scores))}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "daba6f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencySummarizer:\n",
    "    \"\"\"Simple frequency-based summarization\"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor: KhmerTextPreprocessor):\n",
    "        self.preprocessor = preprocessor\n",
    "    \n",
    "    def calculate_word_frequencies(self, words: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate normalized word frequencies using NLTK\"\"\"\n",
    "        freq_dist = FreqDist(words)\n",
    "        max_freq = max(freq_dist.values()) if freq_dist else 1\n",
    "        \n",
    "        normalized_freq = {word: freq / max_freq for word, freq in freq_dist.items()}\n",
    "        return normalized_freq\n",
    "    \n",
    "    def score_sentence(self, sentence: List[str], word_freq: Dict[str, float]) -> float:\n",
    "        \"\"\"Score sentence based on word frequencies\"\"\"\n",
    "        if not sentence:\n",
    "            return 0.0\n",
    "        \n",
    "        score = sum(word_freq.get(word, 0) for word in sentence)\n",
    "        return score / len(sentence)\n",
    "    \n",
    "    def summarize(self, text: str, num_sentences: int = 3, \n",
    "                 summary_ratio: float = None) -> Dict:\n",
    "        \"\"\"Generate summary using word frequency scoring\"\"\"\n",
    "        original_sentences, processed_sentences = self.preprocessor.preprocess_sentences(text)\n",
    "        \n",
    "        if len(original_sentences) == 0:\n",
    "            return {\n",
    "                'summary': \"\",\n",
    "                'method': 'Frequency',\n",
    "                'num_sentences': 0,\n",
    "                'error': 'No sentences found'\n",
    "            }\n",
    "        \n",
    "        if summary_ratio:\n",
    "            num_sentences = max(1, int(len(original_sentences) * summary_ratio))\n",
    "        else:\n",
    "            num_sentences = min(num_sentences, len(original_sentences))\n",
    "        \n",
    "        if len(original_sentences) <= num_sentences:\n",
    "            summary_text = '។ '.join(original_sentences)\n",
    "            return {\n",
    "                'summary': summary_text,\n",
    "                'method': 'Frequency',\n",
    "                'num_sentences': len(original_sentences),\n",
    "                'note': 'Document too short'\n",
    "            }\n",
    "        \n",
    "        all_words = [word for sent in processed_sentences for word in sent]\n",
    "        word_freq = self.calculate_word_frequencies(all_words)\n",
    "        \n",
    "        sentence_scores = [\n",
    "            self.score_sentence(sent, word_freq) \n",
    "            for sent in processed_sentences\n",
    "        ]\n",
    "        \n",
    "        ranked_indices = sorted(\n",
    "            range(len(sentence_scores)), \n",
    "            key=lambda i: sentence_scores[i], \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        selected_indices = sorted(ranked_indices[:num_sentences])\n",
    "        summary_sentences = [original_sentences[i] for i in selected_indices]\n",
    "        \n",
    "        summary_text = '។ '.join(summary_sentences)\n",
    "        summary_text = summary_text.replace('។។', '។')\n",
    "        \n",
    "        return {\n",
    "            'summary': summary_text,\n",
    "            'method': 'Frequency',\n",
    "            'num_sentences': num_sentences,\n",
    "            'total_sentences': len(original_sentences),\n",
    "            'compression_ratio': num_sentences / len(original_sentences),\n",
    "            'sentence_scores': {i: sentence_scores[i] for i in range(len(sentence_scores))}\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b9a3ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationEvaluator:\n",
    "    \"\"\"Evaluate summarization quality using ROUGE scores\"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor: KhmerTextPreprocessor):\n",
    "        self.preprocessor = preprocessor\n",
    "    \n",
    "    def calculate_rouge_scores(self, reference: str, generated: str) -> Dict:\n",
    "        \"\"\"Calculate ROUGE scores\"\"\"\n",
    "        ref_words = self.preprocessor.preprocess_text(reference, remove_stops=False)\n",
    "        gen_words = self.preprocessor.preprocess_text(generated, remove_stops=False)\n",
    "        \n",
    "        if not ref_words or not gen_words:\n",
    "            return {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-l': 0.0}\n",
    "        \n",
    "        # ROUGE-1\n",
    "        ref_unigrams = set(ref_words)\n",
    "        gen_unigrams = set(gen_words)\n",
    "        \n",
    "        overlap_1 = len(ref_unigrams.intersection(gen_unigrams))\n",
    "        rouge_1_precision = overlap_1 / len(gen_unigrams) if gen_unigrams else 0\n",
    "        rouge_1_recall = overlap_1 / len(ref_unigrams) if ref_unigrams else 0\n",
    "        rouge_1_f1 = (2 * rouge_1_precision * rouge_1_recall / \n",
    "                     (rouge_1_precision + rouge_1_recall) \n",
    "                     if (rouge_1_precision + rouge_1_recall) > 0 else 0)\n",
    "        \n",
    "        # ROUGE-2\n",
    "        ref_bigrams = set(zip(ref_words[:-1], ref_words[1:]))\n",
    "        gen_bigrams = set(zip(gen_words[:-1], gen_words[1:]))\n",
    "        \n",
    "        overlap_2 = len(ref_bigrams.intersection(gen_bigrams))\n",
    "        rouge_2_precision = overlap_2 / len(gen_bigrams) if gen_bigrams else 0\n",
    "        rouge_2_recall = overlap_2 / len(ref_bigrams) if ref_bigrams else 0\n",
    "        rouge_2_f1 = (2 * rouge_2_precision * rouge_2_recall / \n",
    "                     (rouge_2_precision + rouge_2_recall) \n",
    "                     if (rouge_2_precision + rouge_2_recall) > 0 else 0)\n",
    "        \n",
    "        # ROUGE-L\n",
    "        def lcs_length(X, Y):\n",
    "            m, n = len(X), len(Y)\n",
    "            L = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "            for i in range(1, m + 1):\n",
    "                for j in range(1, n + 1):\n",
    "                    if X[i-1] == Y[j-1]:\n",
    "                        L[i][j] = L[i-1][j-1] + 1\n",
    "                    else:\n",
    "                        L[i][j] = max(L[i-1][j], L[i][j-1])\n",
    "            return L[m][n]\n",
    "        \n",
    "        lcs_len = lcs_length(ref_words, gen_words)\n",
    "        rouge_l_precision = lcs_len / len(gen_words) if gen_words else 0\n",
    "        rouge_l_recall = lcs_len / len(ref_words) if ref_words else 0\n",
    "        rouge_l_f1 = (2 * rouge_l_precision * rouge_l_recall / \n",
    "                     (rouge_l_precision + rouge_l_recall) \n",
    "                     if (rouge_l_precision + rouge_l_recall) > 0 else 0)\n",
    "        \n",
    "        return {\n",
    "            'rouge-1': {\n",
    "                'precision': rouge_1_precision,\n",
    "                'recall': rouge_1_recall,\n",
    "                'f1': rouge_1_f1\n",
    "            },\n",
    "            'rouge-2': {\n",
    "                'precision': rouge_2_precision,\n",
    "                'recall': rouge_2_recall,\n",
    "                'f1': rouge_2_f1\n",
    "            },\n",
    "            'rouge-l': {\n",
    "                'precision': rouge_l_precision,\n",
    "                'recall': rouge_l_recall,\n",
    "                'f1': rouge_l_f1\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def compression_ratio(self, original: str, summary: str) -> float:\n",
    "        \"\"\"Calculate compression ratio\"\"\"\n",
    "        original_sentences = self.preprocessor.tokenize_sentences(original)\n",
    "        summary_sentences = self.preprocessor.tokenize_sentences(summary)\n",
    "        \n",
    "        if not original_sentences:\n",
    "            return 0.0\n",
    "        \n",
    "        return len(summary_sentences) / len(original_sentences)\n",
    "    \n",
    "    def evaluate_summary(self, original: str, summary: str, \n",
    "                        reference: str = None) -> Dict:\n",
    "        \"\"\"Comprehensive summary evaluation\"\"\"\n",
    "        results = {\n",
    "            'compression_ratio': self.compression_ratio(original, summary),\n",
    "            'original_length': len(self.preprocessor.tokenize_sentences(original)),\n",
    "            'summary_length': len(self.preprocessor.tokenize_sentences(summary))\n",
    "        }\n",
    "        \n",
    "        if reference:\n",
    "            results['rouge_scores'] = self.calculate_rouge_scores(reference, summary)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec61e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KhmerSummarizationSystem:\n",
    "    \"\"\"Main interface for Khmer text summarization\"\"\"\n",
    "    \n",
    "    def __init__(self, stopwords_file: str = \"khmer_stopwords.txt\"):\n",
    "        \"\"\"Initialize the complete summarization system\"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"Initializing Khmer Summarization System...\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        self.preprocessor = KhmerTextPreprocessor(stopwords_file)\n",
    "        self.textrank = TextRankSummarizer(self.preprocessor)\n",
    "        self.tfidf = TFIDFSummarizer(self.preprocessor)\n",
    "        self.frequency = FrequencySummarizer(self.preprocessor)\n",
    "        self.evaluator = SummarizationEvaluator(self.preprocessor)\n",
    "        \n",
    "        print(\"System initialized successfully!\")\n",
    "        print()\n",
    "    \n",
    "    def summarize(self, text: str, method: str = 'textrank', \n",
    "                 num_sentences: int = 3, summary_ratio: float = None) -> Dict:\n",
    "        \"\"\"Generate summary using specified method\"\"\"\n",
    "        method = method.lower()\n",
    "        \n",
    "        if method == 'textrank':\n",
    "            return self.textrank.summarize(text, num_sentences, summary_ratio)\n",
    "        elif method == 'tfidf':\n",
    "            return self.tfidf.summarize(text, num_sentences, summary_ratio)\n",
    "        elif method == 'frequency':\n",
    "            return self.frequency.summarize(text, num_sentences, summary_ratio)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}. Use 'textrank', 'tfidf', or 'frequency'\")\n",
    "    \n",
    "    def summarize_all(self, text: str, num_sentences: int = 3) -> Dict[str, Dict]:\n",
    "        \"\"\"Generate summaries using all methods for comparison\"\"\"\n",
    "        return {\n",
    "            'textrank': self.summarize(text, 'textrank', num_sentences),\n",
    "            'tfidf': self.summarize(text, 'tfidf', num_sentences),\n",
    "            'frequency': self.summarize(text, 'frequency', num_sentences)\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, original: str, summary: str, reference: str = None) -> Dict:\n",
    "        \"\"\"Evaluate summary quality\"\"\"\n",
    "        return self.evaluator.evaluate_summary(original, summary, reference)\n",
    "    \n",
    "    def analyze_document(self, text: str) -> Dict:\n",
    "        \"\"\"Analyze document statistics\"\"\"\n",
    "        sentences = self.preprocessor.tokenize_sentences(text)\n",
    "        words = self.preprocessor.tokenize_words(text)\n",
    "        processed_words = self.preprocessor.preprocess_text(text)\n",
    "        \n",
    "        return {\n",
    "            'num_sentences': len(sentences),\n",
    "            'num_words': len(words),\n",
    "            'num_unique_words': len(set(words)),\n",
    "            'num_processed_words': len(processed_words),\n",
    "            'avg_sentence_length': len(words) / len(sentences) if sentences else 0\n",
    "        }\n",
    "    \n",
    "    def save_summary(self, summary_data: Dict, filename: str):\n",
    "        \"\"\"Save summary to JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Summary saved to {filename}\")\n",
    "    \n",
    "    def load_document(self, filename: str) -> str:\n",
    "        \"\"\"Load document from file\"\"\"\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bad9a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def demo_system():\n",
    "    \"\"\"Demonstration of the Khmer summarization system (writes output to file)\"\"\"\n",
    "    output_lines = []\n",
    "    \n",
    "    output_lines.append(\"=\" * 70)\n",
    "    output_lines.append(\"KHMER TEXT SUMMARIZATION SYSTEM - DEMO\")\n",
    "    output_lines.append(\"=\" * 70)\n",
    "    output_lines.append(\"\")\n",
    "    \n",
    "    # Initialize system\n",
    "    system = KhmerSummarizationSystem()\n",
    "    \n",
    "    # Check for articles directory\n",
    "    articles_dir = os.path.join(\"data\", \"sample_articles\")\n",
    "    results_dir = os.path.join(\"outputs\", \"results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(articles_dir):\n",
    "        output_lines.append(f\"X Directory '{articles_dir}' not found.\")\n",
    "        output_lines.append(\"Please create the directory and add sample articles.\")\n",
    "        write_results(results_dir, \"error_results.txt\", output_lines)\n",
    "        return\n",
    "    \n",
    "    article_files = glob.glob(os.path.join(articles_dir, \"*.txt\"))\n",
    "    if not article_files:\n",
    "        output_lines.append(f\"X No .txt files found in '{articles_dir}'\")\n",
    "        output_lines.append(\"Please add at least one Khmer text file.\")\n",
    "        write_results(results_dir, \"error_results.txt\", output_lines)\n",
    "        return\n",
    "    \n",
    "    # Load the first article\n",
    "    article_file = article_files[0]\n",
    "    article_name = os.path.basename(article_file)\n",
    "    \n",
    "    output_lines.append(f\"Loading article: {article_name}\")\n",
    "    output_lines.append(\"-\" * 70)\n",
    "    \n",
    "    try:\n",
    "        sample_text = system.load_document(article_file)\n",
    "        output_lines.append(f\"Successfully loaded: {article_name}\")\n",
    "        output_lines.append(\"\")\n",
    "    except Exception as e:\n",
    "        output_lines.append(f\"Error loading article: {e}\")\n",
    "        write_results(results_dir, f\"{article_name}_results.txt\", output_lines)\n",
    "        return\n",
    "    \n",
    "    # Document analysis\n",
    "    output_lines.append(\"1. DOCUMENT ANALYSIS\")\n",
    "    output_lines.append(\"-\" * 70)\n",
    "    doc_stats = system.analyze_document(sample_text)\n",
    "    output_lines.append(f\"Sentences: {doc_stats['num_sentences']}\")\n",
    "    output_lines.append(f\"Words: {doc_stats['num_words']}\")\n",
    "    output_lines.append(f\"Unique words: {doc_stats['num_unique_words']}\")\n",
    "    output_lines.append(f\"Average sentence length: {doc_stats['avg_sentence_length']:.1f} words\")\n",
    "    output_lines.append(\"\")\n",
    "    \n",
    "    # Summarization\n",
    "    output_lines.append(\"2. SUMMARIZATION RESULTS\")\n",
    "    output_lines.append(\"-\" * 70)\n",
    "    summaries = system.summarize_all(sample_text, num_sentences=2)\n",
    "    \n",
    "    for method, result in summaries.items():\n",
    "        output_lines.append(f\"\\n{method.upper()} Method:\")\n",
    "        summary_preview = result['summary']\n",
    "        if len(summary_preview) > 200:\n",
    "            summary_preview = summary_preview[:200] + \"...\"\n",
    "        output_lines.append(f\"Summary: {summary_preview}\")\n",
    "        output_lines.append(f\"Compression: {result.get('compression_ratio', 0):.2%}\")\n",
    "    \n",
    "    output_lines.append(\"\")\n",
    "    \n",
    "    # Evaluation\n",
    "    output_lines.append(\"3. EVALUATION EXAMPLE\")\n",
    "    output_lines.append(\"-\" * 70)\n",
    "    textrank_summary = summaries['textrank']['summary']\n",
    "    evaluation = system.evaluate(sample_text, textrank_summary)\n",
    "    output_lines.append(f\"Compression ratio: {evaluation['compression_ratio']:.2%}\")\n",
    "    output_lines.append(f\"Original sentences: {evaluation['original_length']}\")\n",
    "    output_lines.append(f\"Summary sentences: {evaluation['summary_length']}\")\n",
    "    \n",
    "    # ROUGE evaluation with reference\n",
    "    references_dir = os.path.join(\"data\", \"reference_summaries\")\n",
    "    base_name = os.path.splitext(article_name)[0]\n",
    "    reference_file = os.path.join(references_dir, f\"{base_name}.txt\")\n",
    "    \n",
    "    if os.path.exists(reference_file):\n",
    "        output_lines.append(\"\")\n",
    "        output_lines.append(\"4. ROUGE EVALUATION (with reference summary)\")\n",
    "        output_lines.append(\"-\" * 70)\n",
    "        try:\n",
    "            reference_summary = system.load_document(reference_file)\n",
    "            evaluation_with_ref = system.evaluate(sample_text, textrank_summary, reference_summary)\n",
    "            \n",
    "            if 'rouge_scores' in evaluation_with_ref:\n",
    "                rouge_scores = evaluation_with_ref['rouge_scores']\n",
    "                output_lines.append(f\"ROUGE-1 F1: {rouge_scores['rouge-1']['f1']:.3f}\")\n",
    "                output_lines.append(f\"ROUGE-2 F1: {rouge_scores['rouge-2']['f1']:.3f}\")\n",
    "                output_lines.append(f\"ROUGE-L F1: {rouge_scores['rouge-l']['f1']:.3f}\")\n",
    "            else:\n",
    "                output_lines.append(\"No ROUGE scores returned.\")\n",
    "        except Exception as e:\n",
    "            output_lines.append(f\"Error loading reference summary: {e}\")\n",
    "    else:\n",
    "        output_lines.append(\"\")\n",
    "        output_lines.append(f\"Note: Reference summary not found at {reference_file}\")\n",
    "        output_lines.append(\"Create a reference summary to enable ROUGE evaluation.\")\n",
    "    \n",
    "    output_lines.append(\"\")\n",
    "    output_lines.append(\"=\" * 70)\n",
    "    \n",
    "    # Write all results to file\n",
    "    result_filename = f\"{base_name}_results.txt\"\n",
    "    write_results(results_dir, result_filename, output_lines)\n",
    "    print(f\"Results saved to: {os.path.join(results_dir, result_filename)}\")\n",
    "\n",
    "\n",
    "def write_results(directory, filename, lines):\n",
    "    \"\"\"Helper to write output lines to a file.\"\"\"\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4d4d10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Initializing Khmer Summarization System...\n",
      "======================================================================\n",
      "Loaded 100 Khmer stopwords\n",
      "System initialized successfully!\n",
      "\n",
      "Results saved to: outputs\\results\\article_001_results.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "demo_system()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
