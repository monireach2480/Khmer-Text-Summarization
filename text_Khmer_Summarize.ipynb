{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8312ac9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Advanced Khmer Text Processing System Demo ===\n",
      "\n",
      "1. TEXT ANALYSIS\n",
      "--------------------------------------------------\n",
      "Original text length: 313 characters\n",
      "Number of sentences: 2\n",
      "Number of words: 90\n",
      "Processed words: 38\n",
      "Summary: \n",
      "    ក្នុង ឱកាស ទទួល ឯកឧត្តម ផាក ជុង វូក (PARK Jung_Wook) ឯក អគ្គរដ្ឋទូត វិសាមញ្ញ និង ពេញ សមត្ថភាព ន...\n",
      "\n",
      "2. TEXT PREPROCESSING PIPELINE\n",
      "--------------------------------------------------\n",
      "Original sentences: 2\n",
      "Sentence 1: \n",
      "    ក្នុង ឱកាស ទទួល ឯកឧត្តម ផាក ជុង វូក (PARK Jun...\n",
      "Sentence 2: សម្តេច ប្រធាន រដ្ឋសភា បាន សម្តែង នូវ សេចក្តី និង វ...\n",
      "\n",
      "First 10 words: ['ក្នុង', ' ', 'ឱកាស', ' ', 'ទទួល', ' ', 'ឯកឧត្តម', ' ', 'ផាក', ' ']\n",
      "First 10 processed words: ['ក្នុង', 'ឱកាស', 'ទទួល', 'ឯកឧត្តម', 'ផាក', 'ជុង', 'វូក', 'park', 'jung', 'wook']\n",
      "\n",
      "3. TEXT CLASSIFICATION DEMO\n",
      "--------------------------------------------------\n",
      "Test text: ក្រុមបាល់ទាត់បានឈ្នះការប្រកួត\n",
      "Predicted category: កីឡា\n",
      "\n",
      "4. LANGUAGE MODEL DEMO\n",
      "--------------------------------------------------\n",
      "Training 3-gram language model...\n",
      "Model trained on 5 documents\n",
      "Vocabulary size: 67\n",
      "Number of 3-gram contexts: 84\n",
      "Generated text: សម្តេច\n",
      "\n",
      "=== Demo Complete ===\n",
      "\n",
      "=== Additional Features ===\n",
      "\n",
      "Document Analysis:\n",
      "Total sentences: 4\n",
      "Total words: 125\n",
      "Summary (3 sentences):\n",
      "អ្នកជំនាញសុខភាពបន្ថែមថា ការចាក់វ៉ាក់សាំងមានសារៈសំខាន់ក្នុងការកាត់បន្ថយអត្រាឆ្លង និងការរឹតបន្តឹងប្រព័ន្ធការពាររបស់រាងកាយ។ វ៉ាក់សាំងទាំងនេះ នឹងត្រូវចែកចាយទៅតាមមណ្ឌលសុខភាព និងមន្ទីរពេទ្យខេត្តក្រុង ដើម្បី...\n",
      "\n",
      "System initialization complete. Ready for use!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "from math import sqrt, log\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from khmernltk import sentence_tokenize, word_tokenize as khmer_word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KhmerTextPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing pipeline for Khmer text\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stopwords_file=\"stopwords.txt\"):\n",
    "        self.stopwords = self.load_stopwords(stopwords_file)\n",
    "        self.stemmer = PorterStemmer()\n",
    "        \n",
    "    def load_stopwords(self, file_path):\n",
    "        \"\"\"Load Khmer stopwords from file\"\"\"\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                return set(file.read().split(\"\\n\"))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {file_path} not found. Using empty stopwords list.\")\n",
    "            return set()\n",
    "    \n",
    "    def normalize_khmer_text(self, text):\n",
    "        \"\"\"Normalize Khmer text - handle Unicode variations\"\"\"\n",
    "        # Remove extra whitespaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove multiple punctuation\n",
    "        text = re.sub(r'[។]+', '។', text)\n",
    "        text = re.sub(r'[៕]+', '៕', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def remove_punctuation(self, text):\n",
    "        \"\"\"Remove Khmer and English punctuation\"\"\"\n",
    "        khmer_punct = \"។៕៖ៗ៘៙៚៛ៜ៝៞៟\"\n",
    "        english_punct = \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
    "        \n",
    "        for punct in khmer_punct + english_punct:\n",
    "            text = text.replace(punct, \" \")\n",
    "        return text\n",
    "    \n",
    "    def tokenize_sentences(self, text):\n",
    "        \"\"\"Tokenize text into sentences using khmernltk\"\"\"\n",
    "        return sentence_tokenize(text)\n",
    "    \n",
    "    def tokenize_words(self, text):\n",
    "        \"\"\"Tokenize text into words using khmernltk\"\"\"\n",
    "        return khmer_word_tokenize(text)\n",
    "    \n",
    "    def remove_stopwords(self, words):\n",
    "        \"\"\"Remove stopwords from word list\"\"\"\n",
    "        return [word for word in words if word.lower() not in self.stopwords]\n",
    "    \n",
    "    def preprocess_text(self, text, remove_punct=True, remove_stops=True, normalize=True):\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "        if normalize:\n",
    "            text = self.normalize_khmer_text(text)\n",
    "        \n",
    "        if remove_punct:\n",
    "            text = self.remove_punctuation(text)\n",
    "        \n",
    "        words = self.tokenize_words(text)\n",
    "        words = [word.lower() for word in words if len(word.strip()) > 0]\n",
    "        \n",
    "        if remove_stops:\n",
    "            words = self.remove_stopwords(words)\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    def preprocess_document(self, text):\n",
    "        \"\"\"Preprocess entire document maintaining sentence structure\"\"\"\n",
    "        sentences = self.tokenize_sentences(text)\n",
    "        processed_sentences = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            processed_words = self.preprocess_text(sentence)\n",
    "            if processed_words:  # Only add non-empty sentences\n",
    "                processed_sentences.append(processed_words)\n",
    "        \n",
    "        return processed_sentences\n",
    "\n",
    "class KhmerTextSummarizer:\n",
    "    \"\"\"\n",
    "    Extractive text summarization using TextRank algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor):\n",
    "        self.preprocessor = preprocessor\n",
    "    \n",
    "    def cosine_distance(self, u, v):\n",
    "        \"\"\"Calculate cosine distance between two vectors\"\"\"\n",
    "        dot_product = np.dot(u, v)\n",
    "        norm_u = sqrt(np.dot(u, u))\n",
    "        norm_v = sqrt(np.dot(v, v))\n",
    "        \n",
    "        if norm_u == 0 or norm_v == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        return 1 - (dot_product / (norm_u * norm_v))\n",
    "    \n",
    "    def sentence_similarity(self, sent1, sent2):\n",
    "        \"\"\"Calculate similarity between two sentences\"\"\"\n",
    "        all_words = list(set(sent1 + sent2))\n",
    "        \n",
    "        if not all_words:\n",
    "            return 0.0\n",
    "        \n",
    "        vector1 = [0] * len(all_words)\n",
    "        vector2 = [0] * len(all_words)\n",
    "        \n",
    "        for word in sent1:\n",
    "            vector1[all_words.index(word)] += 1\n",
    "        \n",
    "        for word in sent2:\n",
    "            vector2[all_words.index(word)] += 1\n",
    "        \n",
    "        return 1 - self.cosine_distance(vector1, vector2)\n",
    "    \n",
    "    def build_similarity_matrix(self, sentences):\n",
    "        \"\"\"Build sentence similarity matrix\"\"\"\n",
    "        n_sentences = len(sentences)\n",
    "        similarity_matrix = np.zeros((n_sentences, n_sentences))\n",
    "        \n",
    "        for i in range(n_sentences):\n",
    "            for j in range(n_sentences):\n",
    "                if i != j:\n",
    "                    similarity_matrix[i][j] = self.sentence_similarity(\n",
    "                        sentences[i], sentences[j]\n",
    "                    )\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def summarize(self, text, num_sentences=3):\n",
    "        \"\"\"Generate extractive summary\"\"\"\n",
    "        original_sentences = self.preprocessor.tokenize_sentences(text)\n",
    "        processed_sentences = self.preprocessor.preprocess_document(text)\n",
    "        \n",
    "        if len(processed_sentences) <= num_sentences:\n",
    "            return \"។ \".join(original_sentences)\n",
    "        \n",
    "        # Build similarity matrix and apply PageRank\n",
    "        similarity_matrix = self.build_similarity_matrix(processed_sentences)\n",
    "        similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "        scores = nx.pagerank(similarity_graph)\n",
    "        \n",
    "        # Get top sentences\n",
    "        ranked_sentences = sorted(\n",
    "            [(scores[i], i, original_sentences[i]) for i in range(len(original_sentences))],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        summary_sentences = [sent[2] for sent in ranked_sentences[:num_sentences]]\n",
    "        return \"។ \".join(summary_sentences).replace(\"។។\", \"។\")\n",
    "\n",
    "class KhmerTextClassifier:\n",
    "    \"\"\"\n",
    "    Multi-class text classifier for Khmer documents\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.vectorizer = None\n",
    "        self.classifier = None\n",
    "        self.is_trained = False\n",
    "    \n",
    "    def prepare_features(self, texts, fit_vectorizer=False):\n",
    "        \"\"\"Convert texts to feature vectors\"\"\"\n",
    "        processed_texts = []\n",
    "        \n",
    "        for text in texts:\n",
    "            words = self.preprocessor.preprocess_text(text)\n",
    "            processed_texts.append(\" \".join(words))\n",
    "        \n",
    "        if fit_vectorizer or self.vectorizer is None:\n",
    "            self.vectorizer = TfidfVectorizer(\n",
    "                max_features=5000,\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=2,\n",
    "                max_df=0.8\n",
    "            )\n",
    "            features = self.vectorizer.fit_transform(processed_texts)\n",
    "        else:\n",
    "            features = self.vectorizer.transform(processed_texts)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def train(self, texts, labels, model_type='nb'):\n",
    "        \"\"\"Train the classifier\"\"\"\n",
    "        X = self.prepare_features(texts, fit_vectorizer=True)\n",
    "        \n",
    "        if model_type == 'nb':\n",
    "            self.classifier = MultinomialNB()\n",
    "        elif model_type == 'svm':\n",
    "            self.classifier = SVC(kernel='linear', probability=True)\n",
    "        elif model_type == 'lr':\n",
    "            self.classifier = LogisticRegression(max_iter=1000)\n",
    "        \n",
    "        self.classifier.fit(X, labels)\n",
    "        self.is_trained = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, texts):\n",
    "        \"\"\"Predict classes for new texts\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained first\")\n",
    "        \n",
    "        X = self.prepare_features(texts)\n",
    "        return self.classifier.predict(X)\n",
    "    \n",
    "    def predict_proba(self, texts):\n",
    "        \"\"\"Predict class probabilities\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained first\")\n",
    "        \n",
    "        X = self.prepare_features(texts)\n",
    "        return self.classifier.predict_proba(X)\n",
    "    \n",
    "    def evaluate(self, test_texts, test_labels):\n",
    "        \"\"\"Evaluate classifier performance\"\"\"\n",
    "        predictions = self.predict(test_texts)\n",
    "        \n",
    "        accuracy = accuracy_score(test_labels, predictions)\n",
    "        report = classification_report(test_labels, predictions)\n",
    "        conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': conf_matrix\n",
    "        }\n",
    "\n",
    "class KhmerLanguageModel:\n",
    "    \"\"\"\n",
    "    N-gram based language model for Khmer text\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n=3, preprocessor=None):\n",
    "        self.n = n\n",
    "        self.preprocessor = preprocessor or KhmerTextPreprocessor()\n",
    "        self.ngram_counts = defaultdict(Counter)\n",
    "        self.vocabulary = set()\n",
    "        self.is_trained = False\n",
    "    \n",
    "    def train(self, texts):\n",
    "        \"\"\"Train the n-gram language model\"\"\"\n",
    "        print(f\"Training {self.n}-gram language model...\")\n",
    "        \n",
    "        for text in texts:\n",
    "            words = self.preprocessor.preprocess_text(text, remove_stops=False)\n",
    "            self.vocabulary.update(words)\n",
    "            \n",
    "            # Add start and end tokens\n",
    "            padded_words = ['<START>'] * (self.n - 1) + words + ['<END>']\n",
    "            \n",
    "            # Generate n-grams\n",
    "            for i in range(len(padded_words) - self.n + 1):\n",
    "                ngram = tuple(padded_words[i:i + self.n])\n",
    "                context = ngram[:-1]\n",
    "                word = ngram[-1]\n",
    "                self.ngram_counts[context][word] += 1\n",
    "        \n",
    "        self.is_trained = True\n",
    "        print(f\"Model trained on {len(texts)} documents\")\n",
    "        print(f\"Vocabulary size: {len(self.vocabulary)}\")\n",
    "        print(f\"Number of {self.n}-gram contexts: {len(self.ngram_counts)}\")\n",
    "    \n",
    "    def get_probability(self, context, word, smoothing=1e-6):\n",
    "        \"\"\"Calculate probability of word given context with smoothing\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained first\")\n",
    "        \n",
    "        context_tuple = tuple(context)\n",
    "        \n",
    "        if context_tuple not in self.ngram_counts:\n",
    "            return smoothing\n",
    "        \n",
    "        word_count = self.ngram_counts[context_tuple][word]\n",
    "        total_count = sum(self.ngram_counts[context_tuple].values())\n",
    "        \n",
    "        return (word_count + smoothing) / (total_count + len(self.vocabulary) * smoothing)\n",
    "    \n",
    "    def generate_text(self, seed_words=None, max_length=50):\n",
    "        \"\"\"Generate text using the trained model\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained first\")\n",
    "        \n",
    "        if seed_words is None:\n",
    "            context = ['<START>'] * (self.n - 1)\n",
    "        else:\n",
    "            context = seed_words[-(self.n-1):]\n",
    "        \n",
    "        generated = list(context)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            context_tuple = tuple(context)\n",
    "            \n",
    "            if context_tuple not in self.ngram_counts:\n",
    "                break\n",
    "            \n",
    "            # Sample next word based on probability distribution\n",
    "            candidates = list(self.ngram_counts[context_tuple].keys())\n",
    "            weights = [self.ngram_counts[context_tuple][word] for word in candidates]\n",
    "            \n",
    "            if '<END>' in candidates and np.random.random() < 0.1:\n",
    "                break\n",
    "            \n",
    "            next_word = np.random.choice(candidates, p=np.array(weights)/sum(weights))\n",
    "            \n",
    "            if next_word == '<END>':\n",
    "                break\n",
    "            \n",
    "            generated.append(next_word)\n",
    "            context = context[1:] + [next_word]\n",
    "        \n",
    "        return ' '.join([word for word in generated if word not in ['<START>', '<END>']])\n",
    "    \n",
    "    def calculate_perplexity(self, test_texts):\n",
    "        \"\"\"Calculate perplexity on test texts\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained first\")\n",
    "        \n",
    "        total_log_prob = 0\n",
    "        total_words = 0\n",
    "        \n",
    "        for text in test_texts:\n",
    "            words = self.preprocessor.preprocess_text(text, remove_stops=False)\n",
    "            padded_words = ['<START>'] * (self.n - 1) + words + ['<END>']\n",
    "            \n",
    "            for i in range(len(padded_words) - self.n + 1):\n",
    "                ngram = padded_words[i:i + self.n]\n",
    "                context = ngram[:-1]\n",
    "                word = ngram[-1]\n",
    "                \n",
    "                prob = self.get_probability(context, word)\n",
    "                total_log_prob += log(prob)\n",
    "                total_words += 1\n",
    "        \n",
    "        avg_log_prob = total_log_prob / total_words\n",
    "        perplexity = np.exp(-avg_log_prob)\n",
    "        \n",
    "        return perplexity\n",
    "\n",
    "class KhmerNLP:\n",
    "    \"\"\"\n",
    "    Main class integrating all NLP components\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stopwords_file=\"stopwords.txt\"):\n",
    "        self.preprocessor = KhmerTextPreprocessor(stopwords_file)\n",
    "        self.summarizer = KhmerTextSummarizer(self.preprocessor)\n",
    "        self.classifier = KhmerTextClassifier(self.preprocessor)\n",
    "        self.language_model = KhmerLanguageModel(n=3, preprocessor=self.preprocessor)\n",
    "    \n",
    "    def analyze_text(self, text):\n",
    "        \"\"\"Comprehensive text analysis\"\"\"\n",
    "        result = {\n",
    "            'original_text': text,\n",
    "            'sentences': self.preprocessor.tokenize_sentences(text),\n",
    "            'words': self.preprocessor.tokenize_words(text),\n",
    "            'processed_words': self.preprocessor.preprocess_text(text),\n",
    "            'word_count': len(self.preprocessor.tokenize_words(text)),\n",
    "            'sentence_count': len(self.preprocessor.tokenize_sentences(text)),\n",
    "            'summary': self.summarizer.summarize(text, num_sentences=2)\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def save_models(self, base_path=\"models\"):\n",
    "        \"\"\"Save trained models\"\"\"\n",
    "        import os\n",
    "        os.makedirs(base_path, exist_ok=True)\n",
    "        \n",
    "        # Save classifier\n",
    "        if self.classifier.is_trained:\n",
    "            with open(f\"{base_path}/classifier.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.classifier, f)\n",
    "        \n",
    "        # Save language model\n",
    "        if self.language_model.is_trained:\n",
    "            with open(f\"{base_path}/language_model.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.language_model, f)\n",
    "    \n",
    "    def load_models(self, base_path=\"models\"):\n",
    "        \"\"\"Load pre-trained models\"\"\"\n",
    "        try:\n",
    "            with open(f\"{base_path}/classifier.pkl\", \"rb\") as f:\n",
    "                self.classifier = pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(\"Classifier model not found\")\n",
    "        \n",
    "        try:\n",
    "            with open(f\"{base_path}/language_model.pkl\", \"rb\") as f:\n",
    "                self.language_model = pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(\"Language model not found\")\n",
    "\n",
    "def demo_system():\n",
    "    \"\"\"\n",
    "    Demonstration of the complete system\n",
    "    \"\"\"\n",
    "    print(\"=== Advanced Khmer Text Processing System Demo ===\\n\")\n",
    "    \n",
    "    # Initialize system\n",
    "    nlp_system = KhmerNLP()\n",
    "    \n",
    "    # Sample Khmer text for demonstration\n",
    "    sample_text = \"\"\"\n",
    "    ក្នុង ឱកាស ទទួល ឯកឧត្តម ផាក ជុង វូក (PARK Jung_Wook) ឯក អគ្គរដ្ឋទូត វិសាមញ្ញ និង ពេញ សមត្ថភាព នៃ សាធារណរដ្ឋ កូរ៉េ ប្រចាំ ព្រះរាជាណាចក្រ កម្ពុជា ចូល ជួប សម្តែង កិត្តិយស ។ សម្តេច ប្រធាន រដ្ឋសភា បាន សម្តែង នូវ សេចក្តី និង វាយតម្លៃ ត្រឹមត្រូវ ចំពោះ ទំនាក់ទំនង និង កិច្ចសហប្រតិបត្តិការ រវាង កម្ពុជា - កូរ៉េ ។\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Text Analysis Demo\n",
    "    print(\"1. TEXT ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    analysis = nlp_system.analyze_text(sample_text)\n",
    "    print(f\"Original text length: {len(analysis['original_text'])} characters\")\n",
    "    print(f\"Number of sentences: {analysis['sentence_count']}\")\n",
    "    print(f\"Number of words: {analysis['word_count']}\")\n",
    "    print(f\"Processed words: {len(analysis['processed_words'])}\")\n",
    "    print(f\"Summary: {analysis['summary'][:100]}...\")\n",
    "    print()\n",
    "    \n",
    "    # 2. Text Preprocessing Demo\n",
    "    print(\"2. TEXT PREPROCESSING PIPELINE\")\n",
    "    print(\"-\" * 50)\n",
    "    sentences = nlp_system.preprocessor.tokenize_sentences(sample_text)\n",
    "    print(f\"Original sentences: {len(sentences)}\")\n",
    "    for i, sent in enumerate(sentences[:2], 1):\n",
    "        print(f\"Sentence {i}: {sent[:50]}...\")\n",
    "    \n",
    "    words = nlp_system.preprocessor.tokenize_words(sample_text)\n",
    "    print(f\"\\nFirst 10 words: {words[:10]}\")\n",
    "    \n",
    "    processed_words = nlp_system.preprocessor.preprocess_text(sample_text)\n",
    "    print(f\"First 10 processed words: {processed_words[:10]}\")\n",
    "    print()\n",
    "    \n",
    "    # 3. Demonstrate classifier training (with synthetic data)\n",
    "    print(\"3. TEXT CLASSIFICATION DEMO\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create sample training data\n",
    "    sample_texts = [\n",
    "        \"នេះជាអត្ថបទអំពីនយោបាយ រដ្ឋាភិបាលបានធ្វើការកែទម្រង់\",\n",
    "        \"ប្រធានក្រុមហ៊ុនបានប្រកាសពីផលិតផលថ្មី នេះជាការរីកចំរើនដ៏ល្អ\",\n",
    "        \"កីឡាករបានឈ្នះការប្រកួត ក្រុមជាតិបានរកបានជ័យជំនះ\",\n",
    "        \"រដ្ឋាភិបាលបានអនុម័តច្បាប់ថ្មី នយោបាយនេះនឹងប្តូរសង្គម\"\n",
    "    ]\n",
    "    sample_labels = [\"នយោបាយ\", \"អាជីវកម្ម\", \"កីឡា\", \"នយោបាយ\"]\n",
    "    \n",
    "    # Train classifier\n",
    "    nlp_system.classifier.train(sample_texts, sample_labels, model_type='nb')\n",
    "    \n",
    "    # Test prediction\n",
    "    test_text = \"ក្រុមបាល់ទាត់បានឈ្នះការប្រកួត\"\n",
    "    prediction = nlp_system.classifier.predict([test_text])\n",
    "    print(f\"Test text: {test_text}\")\n",
    "    print(f\"Predicted category: {prediction[0]}\")\n",
    "    print()\n",
    "    \n",
    "    # 4. Language Model Demo\n",
    "    print(\"4. LANGUAGE MODEL DEMO\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train language model\n",
    "    training_texts = [sample_text] + sample_texts\n",
    "    nlp_system.language_model.train(training_texts)\n",
    "    \n",
    "    # Generate text\n",
    "    generated = nlp_system.language_model.generate_text(\n",
    "        seed_words=[\"សម្តេច\"], max_length=10\n",
    "    )\n",
    "    print(f\"Generated text: {generated}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=== Demo Complete ===\")\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Run demonstration\n",
    "    demo_system()\n",
    "    \n",
    "    # Additional functionality examples\n",
    "    print(\"\\n=== Additional Features ===\")\n",
    "    \n",
    "    # Create system instance\n",
    "    khmer_nlp = KhmerNLP()\n",
    "    \n",
    "    # Example of reading from file and summarizing\n",
    "    try:\n",
    "        with open(\"Khmer.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        # Analyze document\n",
    "        analysis = khmer_nlp.analyze_text(content)\n",
    "        print(f\"\\nDocument Analysis:\")\n",
    "        print(f\"Total sentences: {analysis['sentence_count']}\")\n",
    "        print(f\"Total words: {analysis['word_count']}\")\n",
    "        print(f\"Summary (3 sentences):\")\n",
    "        summary = khmer_nlp.summarizer.summarize(content, num_sentences=3)\n",
    "        print(summary[:200] + \"...\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Khmer.txt file not found - create sample file for testing\")\n",
    "    \n",
    "    print(\"\\nSystem initialization complete. Ready for use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
