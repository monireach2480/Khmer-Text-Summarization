{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_md",
   "metadata": {},
   "source": [
    "# ITM 454: Khmer News Summarizer - Final Project\n",
    "\n",
    "This notebook implements an extractive text summarizer for Khmer news articles. The process includes:\n",
    "1.  **Data Collection**: Scraping a live news article from the web.\n",
    "2.  **Preprocessing**: Cleaning and preparing the Khmer text for analysis.\n",
    "3.  **Summarization**: Applying the TextRank algorithm to extract key sentences.\n",
    "4.  **Evaluation**: Measuring the summary quality with ROUGE scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "setup_imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported (with fallbacks if needed).\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup and Imports\n",
    "# If libraries are missing, install via: !pip install -r requirements.txt\n",
    "\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Khmer tokenization (preferred). Fallbacks are provided below.\n",
    "try:\n",
    "    from khmernltk import word_tokenize as km_word_tokenize\n",
    "    from khmernltk import sentence_tokenize as km_sentence_tokenize\n",
    "except Exception:\n",
    "    km_word_tokenize = None\n",
    "    km_sentence_tokenize = None\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "except Exception:\n",
    "    rouge_scorer = None\n",
    "\n",
    "print('âœ… Libraries imported (with fallbacks if needed).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "tokenizers_util",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Khmer tokenization utilities and preprocessing helpers ---\n",
    "\n",
    "# Simple Khmer sentence splitter fallback (splits on Khmer punctuation)\n",
    "SENT_SPLIT_REGEX = re.compile(r'[áŸ”!?áŸ–]+[\"]?\\s*')\n",
    "\n",
    "# Remove non-Khmer letters/numbers/some punctuation while keeping spaces\n",
    "# Line 34: Correct\n",
    "NON_KHMER_CLEAN = re.compile(r\"[a-zA-Z0-9\\.,!?\\(\\)\\[\\]\\{\\}\\\"':;]\")\n",
    "\n",
    "# A minimal Khmer stopword list (extend as needed)\n",
    "KHMER_STOPWORDS = set([\n",
    "    'á“á·á„', 'á“áŸƒ', 'á€áŸ’á“á»á„', 'á‡á¶', 'á“áŸ…', 'á”á¶á“', 'áá¶', 'áŠáŸ„á™', 'áŠáŸ‚áš', 'á‘áŸ…', 'á²áŸ’á™', 'á–á¸',\n",
    "    'á˜á½á™', 'áŸ—', 'áŠáŸ‚á›', 'á˜á¶á“', 'á‘áŸ', 'á“áŸáŸ‡', 'á“áŸ„áŸ‡', 'á‚áº', 'á€áŸ', 'á›áŸ„á€', 'á¢áŸ’á“á€', 'á™á¾á„',\n",
    "    'á‚áŸ', 'á‚á¶ááŸ‹', 'ááŸ’á‰á»áŸ†', 'áœá¶'\n",
    "])\n",
    "\n",
    "def khmer_sentence_tokenize(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    if km_sentence_tokenize is not None:\n",
    "        try:\n",
    "            return [s.strip() for s in km_sentence_tokenize(text) if s.strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Fallback: regex-based split\n",
    "    return [p.strip() for p in SENT_SPLIT_REGEX.split(text) if p and p.strip()]\n",
    "\n",
    "def khmer_word_tokenize(sent: str):\n",
    "    if not sent:\n",
    "        return []\n",
    "    if km_word_tokenize is not None:\n",
    "        try:\n",
    "            return km_word_tokenize(sent, keep_whitespace=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Fallback: whitespace split; if no spaces, split into characters\n",
    "    return sent.split() if ' ' in sent else list(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scraper_md",
   "metadata": {},
   "source": [
    "## Section 2: Data Collection (Web Scraper)\n",
    "\n",
    "This function scrapes the title and content of a news article from a given URL. It is designed for Khmer news sites and includes robust fallbacks (AMP, proxy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "scraper_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _request_with_retries(url, headers=None, timeout=10, retries=3, backoff=1.3):\n",
    "    headers = headers or {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'km-KH,km;q=0.9,en;q=0.8'\n",
    "    }\n",
    "    last_exc = None\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "            if resp.status_code == 200:\n",
    "                return resp\n",
    "            else:\n",
    "                last_exc = Exception(f'HTTP {resp.status_code}')\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "        time.sleep(backoff ** attempt)\n",
    "    if last_exc:\n",
    "        print(f'Request failed after {retries} attempts: {last_exc}')\n",
    "    return None\n",
    "\n",
    "def scrape_vod_article(url):\n",
    "    \"\"\"\n",
    "    Scrapes the title and article text from a Khmer news URL (with fallbacks).\n",
    "    Returns: (title, text) or (None, None)\n",
    "    \"\"\"\n",
    "    # 1) Try direct fetch\n",
    "    response = _request_with_retries(url, timeout=10, retries=3, backoff=1.3)\n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title_el = soup.find('h1', class_='entry-title') or soup.find('h1')\n",
    "        title_text = title_el.get_text(strip=True) if title_el else None\n",
    "        content_div = soup.find('div', class_='entry-content')\n",
    "        paragraphs = []\n",
    "        if content_div:\n",
    "            paragraphs = content_div.find_all('p')\n",
    "        if not paragraphs:\n",
    "            paragraphs = soup.find_all('p')\n",
    "        article_text = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "        article_text = re.sub(r'\\s+', ' ', article_text).strip()\n",
    "        if article_text:\n",
    "            print(f\"Scraping successful for: {title_text or url}\")\n",
    "            return title_text, article_text\n",
    "\n",
    "    # 2) Try AMP page variants\n",
    "    amp_candidates = [url.rstrip('/') + '/amp/', url.rstrip('/') + '/amp']\n",
    "    for amp_url in amp_candidates:\n",
    "        response = _request_with_retries(amp_url, timeout=10, retries=2, backoff=1.2)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title_el = soup.find('h1')\n",
    "            title_text = title_el.get_text(strip=True) if title_el else None\n",
    "            paragraphs = soup.find_all('p')\n",
    "            article_text = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "            article_text = re.sub(r'\\s+', ' ', article_text).strip()\n",
    "            if article_text:\n",
    "                print(f\"Scraped AMP page for: {title_text or url}\")\n",
    "                return title_text, article_text\n",
    "\n",
    "    # 3) Try r.jina.ai content extraction proxy\n",
    "    try:\n",
    "        scheme_split = url.split('://', 1)\n",
    "        if len(scheme_split) == 2:\n",
    "            rjina_url = f\"https://r.jina.ai/http://{scheme_split[1]}\"\n",
    "        else:\n",
    "            rjina_url = f\"https://r.jina.ai/http://{url}\"\n",
    "        r = _request_with_retries(rjina_url, timeout=10, retries=2, backoff=1.2)\n",
    "        if r and r.status_code == 200:\n",
    "            text = r.text.strip()\n",
    "            if text:\n",
    "                # Heuristic: first non-empty line as title\n",
    "                lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "                title_text = lines[0] if lines else None\n",
    "                print(f\"Fetched via r.jina.ai proxy: {title_text or url}\")\n",
    "                return title_text, text\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print('Error: Could not fetch article content from the URL or fallbacks.')\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocess_md",
   "metadata": {},
   "source": [
    "## Section 3: Text Preprocessing Pipeline\n",
    "\n",
    "Cleans and prepares Khmer text for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "preprocess_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_khmer_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses Khmer text for summarization.\n",
    "    Returns:\n",
    "      - original_sentences: list of raw sentences\n",
    "      - processed_sentences: list of tokenized/cleaned sentences used for modeling\n",
    "      - idx_map: list mapping processed_sentences index -> original_sentences index\n",
    "    \"\"\"\n",
    "    sentences = khmer_sentence_tokenize(text)\n",
    "\n",
    "    processed_sentences = []\n",
    "    idx_map = []\n",
    "    for i, sent in enumerate(sentences):\n",
    "        sent_clean = NON_KHMER_CLEAN.sub('', sent).strip()\n",
    "        tokens = khmer_word_tokenize(sent_clean)\n",
    "        filtered_tokens = [w for w in tokens if w not in KHMER_STOPWORDS and len(w) > 1]\n",
    "        if filtered_tokens:\n",
    "            processed_sentences.append(' '.join(filtered_tokens))\n",
    "            idx_map.append(i)\n",
    "\n",
    "    return sentences, processed_sentences, idx_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summarize_md",
   "metadata": {},
   "source": [
    "## Section 4: The Summarization Model (TextRank)\n",
    "\n",
    "Represent each sentence as a TF-IDF vector, compute similarities, apply PageRank, and select top sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "summarize_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_textrank(original_sentences, processed_sentences, idx_map, num_sentences=3):\n",
    "    if not processed_sentences:\n",
    "        return 'Could not generate summary. No content after preprocessing.'\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n",
    "\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    ranked = sorted(((scores[i], idx_map[i]) for i in range(len(idx_map))), reverse=True)\n",
    "    num_sentences = max(1, min(num_sentences, len(ranked)))\n",
    "    top = sorted(ranked[:num_sentences], key=lambda x: x[1])\n",
    "    return ' '.join([original_sentences[j] for _, j in top])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo_md",
   "metadata": {},
   "source": [
    "## Section 5: Putting It All Together (Live Demo)\n",
    "\n",
    "Runs the entire pipeline on a live Khmer news article. If fetching fails, it will try AMP, a text extraction proxy, a local file, or prompt for manual paste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "demo_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to scrape: https://khmernews.news/article/archives/141231\n",
      "Scraping successful for: https://khmernews.news/article/archives/141231\n",
      "\n",
      "==================================================\n",
      "ğŸ“° ARTICLE TITLE: None\n",
      "==================================================\n",
      "\n",
      "âœ¨ GENERATED SUMMARY:\n",
      "á”áŸ’ášá’á¶á“á¶á’á·á”áá¸á”á¶ášá¶áŸ†á„ á›áŸ„á€ Emmanuel Macron á”á¶á“á“á·á™á¶á™á“áŸ…ááŸ’á„áŸƒá–áŸ’ášá áŸáŸ’á”áá·áŸáŸá”áŸ’áŠá¶á áŸá“áŸáŸ‡ á“áŸ…á›á¾á”ááŸ’áŠá¶á‰áŸá„áŸ’á‚á˜ X áá¶ á”áŸ’ášá‘áŸáŸá”á¶ášá¶áŸ†á„á“á¹á„áŠá¶á€áŸ‹á–á„áŸ’ášá¶á™á™á“áŸ’áá áŸ„áŸ‡á…á˜áŸ’á”á¶áŸ†á„ Rafale á…áŸ†á“á½á“ áŸ£ á‚áŸ’ášá¿á„ áŠá¾á˜áŸ’á”á¸á‡á½á™á”áŸ‰á¼á¡á¼á‰á€á¶ášá–á¶ášáŠáŸ‚á“á¢á¶á€á¶áŸ á”á“áŸ’á‘á¶á”áŸ‹á–á¸á”áŸ’ášá‘áŸáŸáŸá˜á¶á‡á·á€á¢á„áŸ’á‚á€á¶ášáá¶áá¼á˜á½á™á“áŸáŸ‡á”á¶á“ášá„á€á¶ášá›á»á€á›á»á™á–á¸áŸáŸ†áá¶á€áŸ‹á™á“áŸ’áá áŸ„áŸ‡á‚áŸ’á˜á¶á“á˜á“á»áŸáŸ’áŸá”á¾á€ášá”áŸáŸ‹ášá»áŸáŸ’áŸáŸŠá¸á€áŸ’á“á»á„áŸá”áŸ’áá¶á áŸá“áŸáŸ‡áŸ” á‚á·áááŸ’ášá¹á˜ááŸ’á„áŸƒáŸá»á€áŸ’ášá“áŸáŸ‡ á”áŸ’ášá‘áŸáŸáŸá˜áŸ’á–áŸá“áŸ’á’á˜á·ááŸ’áá¢á„áŸ’á‚á€á¶áš áá¶áá¼ á€áŸ†á–á»á„áŸáŸ’áá·áá“áŸ…á€áŸ’á“á»á„áŸáŸ’áá¶á“á—á¶á–á”áŸ’ášáˆá˜á»áá“á¹á„ášá»áŸáŸ’áŸáŸŠá¸á€á¶á“áŸ‹ááŸ‚ááŸ’á›á¶áŸ†á„ áááŸˆáŠáŸ‚á›ášá»áŸáŸ’áŸáŸŠá¸ á“á·á„á”áŸ’ášá‘áŸáŸâ€‹á”áŸŠáŸá¡á¶ášá»áŸ á€áŸ†á–á»á„á’áŸ’áœá¾áŸá˜á™á»á‘áŸ’á’á™áŸ„á’á¶á“áŸ…á‡á¶á”áŸ‹á“á¹á„á˜á¶ááŸ‹á‘áŸ’áœá¶ášá•áŸ’á‘áŸ‡ášá”áŸáŸ‹áá¶áá¼áŸ” [email protected]070 878884 / 078 878884 á¢á¶áŸáŸá™áŠáŸ’á‹á¶á“áŸ–\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "\n",
    "# Example URL (you may replace with any Khmer news URL)\n",
    "ARTICLE_URL = 'https://khmernews.news/article/archives/141231'\n",
    "# Optional local text file fallback (set to a valid path to auto-load)\n",
    "LOCAL_TEXT_FILE = ''  # e.g., r'c:\\path\\to\\article.txt'\n",
    "\n",
    "print(f'Attempting to scrape: {ARTICLE_URL}')\n",
    "title, article_text = scrape_vod_article(ARTICLE_URL)\n",
    "\n",
    "# Local file fallback\n",
    "if not article_text and LOCAL_TEXT_FILE:\n",
    "    try:\n",
    "        with open(LOCAL_TEXT_FILE, 'r', encoding='utf-8') as f:\n",
    "            article_text = f.read().strip()\n",
    "        title = title or 'Local file'\n",
    "        print(f'Loaded article text from: {LOCAL_TEXT_FILE}')\n",
    "    except Exception as e:\n",
    "        print(f'Failed to read LOCAL_TEXT_FILE: {e}')\n",
    "\n",
    "# Manual paste fallback\n",
    "if not article_text:\n",
    "    print('Fetching failed. Paste Khmer article text below to proceed.')\n",
    "    try:\n",
    "        article_text = input('Paste article text (or leave blank to cancel): ').strip()\n",
    "    except Exception:\n",
    "        article_text = ''\n",
    "    if article_text:\n",
    "        title = title or 'Manual input'\n",
    "\n",
    "if article_text:\n",
    "    original_sentences, processed_sentences, idx_map = preprocess_khmer_text(article_text)\n",
    "    summary = summarize_textrank(original_sentences, processed_sentences, idx_map, num_sentences=3)\n",
    "\n",
    "    print('\\n' + '='*50)\n",
    "    print(f'ğŸ“° ARTICLE TITLE: {title}')\n",
    "    print('='*50 + '\\n')\n",
    "    print('âœ¨ GENERATED SUMMARY:')\n",
    "    print(summary)\n",
    "    print('\\n' + '='*50)\n",
    "else:\n",
    "    print('No article text available. Skipping summarization.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_md",
   "metadata": {},
   "source": [
    "## Section 6: Evaluation (ROUGE Score)\n",
    "\n",
    "Compare the generated summary against a baseline (first 3 sentences). For Khmer, stemming is disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eval_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š EVALUATION (vs. first 3 sentences):\n",
      "  - ROUGE-1 (Overlap of unigrams): 0.9624\n",
      "  - ROUGE-2 (Overlap of bigrams): 0.9612\n",
      "  - ROUGE-L (Longest common subsequence): 0.9203\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "if 'summary' in globals() and 'article_text' in globals() and article_text:\n",
    "    reference_summary = ' '.join(original_sentences[:3])\n",
    "    if rouge_scorer is not None:\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "        scores = scorer.score(reference_summary, summary)\n",
    "        print('\\nğŸ“Š EVALUATION (vs. first 3 sentences):')\n",
    "        print(f\"  - ROUGE-1 (Overlap of unigrams): {scores['rouge1'].fmeasure:.4f}\")\n",
    "        print(f\"  - ROUGE-2 (Overlap of bigrams): {scores['rouge2'].fmeasure:.4f}\")\n",
    "        print(f\"  - ROUGE-L (Longest common subsequence): {scores['rougeL'].fmeasure:.4f}\")\n",
    "        print('\\n' + '='*50)\n",
    "    else:\n",
    "        print('rouge-score not installed/importable. Install it to run evaluation.')\n",
    "else:\n",
    "    print('Evaluation skipped: no summary available.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
