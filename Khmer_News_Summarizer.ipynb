{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_md",
   "metadata": {},
   "source": [
    "# ITM 454: Khmer News Summarizer - Final Project\n",
    "\n",
    "This notebook implements:\n",
    "- A Khmer extractive text summarizer (TextRank).\n",
    "- A Machine Learning text classification demo using NLTK (Naive Bayes).\n",
    "\n",
    "Pipeline:\n",
    "1.  Data Collection (scraping with robust fallbacks)\n",
    "2.  Preprocessing (Khmer-aware tokenization and normalization)\n",
    "3.  Summarization (TextRank)\n",
    "4.  Evaluation (ROUGE)\n",
    "5.  ML Implementation (NLTK Naive Bayes text classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "setup_imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup and Imports\n",
    "# Install necessary libraries if you haven't already using requirements.txt\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from khmernltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# NLTK's sentence tokenizer is generally language-agnostic and works well for sentence boundaries.\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "tokenizers_util",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_vod_article(url):\n",
    "    \"\"\"\n",
    "    Scrapes the title and article text from a VOD Khmer news URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: Failed to fetch URL with status code {response.status_code}\")\n",
    "            return None, None\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the article title\n",
    "        title = soup.find('h1', class_='entry-title')\n",
    "        if not title:\n",
    "            print(\"Error: Could not find the article title.\")\n",
    "            return None, None\n",
    "        title_text = title.get_text().strip()\n",
    "\n",
    "        # Find the main article content\n",
    "        content_div = soup.find('div', class_='entry-content')\n",
    "        if not content_div:\n",
    "            print(\"Error: Could not find the article content.\")\n",
    "            return None, None\n",
    "            \n",
    "        paragraphs = content_div.find_all('p')\n",
    "        article_text = ' '.join([p.get_text().strip() for p in paragraphs])\n",
    "        \n",
    "        print(f\" Scraping successful for: {title_text}\")\n",
    "        return title_text, article_text\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred during scraping: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scraper_md",
   "metadata": {},
   "source": [
    "## Section 2: Data Collection (Web Scraper)\n",
    "\n",
    "Robust scraping with headers, retries, AMP fallback and r.jina.ai proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "scraper_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _request_with_retries(url, headers=None, timeout=10, retries=3, backoff=1.3):\n",
    "    headers = headers or {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'km-KH,km;q=0.9,en;q=0.8'\n",
    "    }\n",
    "    last_exc = None\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "            if resp.status_code == 200:\n",
    "                return resp\n",
    "            else:\n",
    "                last_exc = Exception(f'HTTP {resp.status_code}')\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "        time.sleep(backoff ** attempt)\n",
    "    if last_exc:\n",
    "        print(f'Request failed after {retries} attempts: {last_exc}')\n",
    "    return None\n",
    "\n",
    "def scrape_vod_article(url):\n",
    "    \"\"\"\n",
    "    Scrapes the title and article text from a Khmer news URL (with fallbacks).\n",
    "    Returns: (title, text) or (None, None)\n",
    "    \"\"\"\n",
    "    # 1) Try direct fetch\n",
    "    response = _request_with_retries(url, timeout=10, retries=3, backoff=1.3)\n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title_el = soup.find('h1', class_='entry-title') or soup.find('h1')\n",
    "        title_text = title_el.get_text(strip=True) if title_el else None\n",
    "        content_div = soup.find('div', class_='entry-content')\n",
    "        paragraphs = []\n",
    "        if content_div:\n",
    "            paragraphs = content_div.find_all('p')\n",
    "        if not paragraphs:\n",
    "            paragraphs = soup.find_all('p')\n",
    "        article_text = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "        article_text = re.sub(r'\\s+', ' ', article_text).strip()\n",
    "        if article_text:\n",
    "            print(f\"Scraping successful for: {title_text or url}\")\n",
    "            return title_text, article_text\n",
    "\n",
    "    # 2) Try AMP page variants\n",
    "    amp_candidates = [url.rstrip('/') + '/amp/', url.rstrip('/') + '/amp']\n",
    "    for amp_url in amp_candidates:\n",
    "        response = _request_with_retries(amp_url, timeout=10, retries=2, backoff=1.2)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title_el = soup.find('h1')\n",
    "            title_text = title_el.get_text(strip=True) if title_el else None\n",
    "            paragraphs = soup.find_all('p')\n",
    "            article_text = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "            article_text = re.sub(r'\\s+', ' ', article_text).strip()\n",
    "            if article_text:\n",
    "                print(f\"Scraped AMP page for: {title_text or url}\")\n",
    "                return title_text, article_text\n",
    "\n",
    "    # 3) Try r.jina.ai content extraction proxy\n",
    "    try:\n",
    "        scheme_split = url.split('://', 1)\n",
    "        if len(scheme_split) == 2:\n",
    "            rjina_url = f\"https://r.jina.ai/http://{scheme_split[1]}\"\n",
    "        else:\n",
    "            rjina_url = f\"https://r.jina.ai/http://{url}\"\n",
    "        r = _request_with_retries(rjina_url, timeout=10, retries=2, backoff=1.2)\n",
    "        if r and r.status_code == 200:\n",
    "            text = r.text.strip()\n",
    "            if text:\n",
    "                lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "                title_text = lines[0] if lines else None\n",
    "                print(f\"Fetched via r.jina.ai proxy: {title_text or url}\")\n",
    "                return title_text, text\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print('Error: Could not fetch article content from the URL or fallbacks.')\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocess_md",
   "metadata": {},
   "source": [
    "## Section 3: Text Preprocessing Pipeline\n",
    "\n",
    "Cleans and prepares Khmer text for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "preprocess_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom list of Khmer stopwords. A more comprehensive list would improve results.\n",
    "KHMER_STOPWORDS = [\n",
    "    'á“á·á„', 'á“áŸƒ', 'á€áŸ’á“á»á„', 'á‡á¶', 'á“áŸ…', 'á”á¶á“', 'áá¶', 'áŠáŸ„á™', 'áŠáŸ‚áš', 'ã•ã‚“ã‚‚',\n",
    "    'á‘áŸ…', 'á²áŸ’á™', 'á–á¸', 'á˜á½á™', 'áŸ—', 'áŸ”', 'áŸ—', 'áŠáŸ‚á›', 'á˜á¶á“', 'à¸à¸²à¸£', 'à¸™à¸µà¹‰',\n",
    "    'á›áŸ„á€', 'á¢áŸ’á“á€', 'ááŸ’á‰á»áŸ†', 'á‚áŸ', 'á™á¾á„', 'áœá¶', 'á‚á¶ááŸ‹', 'à¸™à¸±à¹‰à¸™', 'á“áŸáŸ‡', 'á‘áŸ', 'áŠáŸ‚áš'\n",
    "]\n",
    "\n",
    "def preprocess_khmer_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses Khmer text for summarization.\n",
    "    \"\"\"\n",
    "    # 1. Sentence Segmentation\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # 2. Clean and tokenize each sentence\n",
    "    processed_sentences = []\n",
    "    for sent in sentences:\n",
    "        # Lowercase and remove non-Khmer characters/punctuation (keeping spaces)\n",
    "        sent = re.sub(r'[a-zA-Z0-9\\.,!?\\(\\)\\[\\]\\{\\}\"\\':;]', '', sent)\n",
    "        sent = sent.strip()\n",
    "        \n",
    "        # 3. Word Tokenization using khmernltk\n",
    "        tokens = word_tokenize(sent)\n",
    "        \n",
    "        # 4. Stopword Removal\n",
    "        filtered_tokens = [word for word in tokens if word not in KHMER_STOPWORDS and len(word) > 1]\n",
    "        \n",
    "        if filtered_tokens:\n",
    "            processed_sentences.append(\" \".join(filtered_tokens))\n",
    "            \n",
    "    return sentences, processed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summarize_md",
   "metadata": {},
   "source": [
    "## Section 4: The Summarization Model (TextRank)\n",
    "\n",
    "Represent each sentence as a TF-IDF vector, compute similarities, apply PageRank, and select top sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "summarize_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_textrank(original_sentences, processed_sentences, num_sentences=3):\n",
    "    \"\"\"\n",
    "    Generates a summary using the TextRank algorithm.\n",
    "    \"\"\"\n",
    "    if not processed_sentences:\n",
    "        return \"Could not generate summary. No content after preprocessing.\"\n",
    "\n",
    "    # 1. Vectorize sentences using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n",
    "\n",
    "    # 2. Calculate sentence similarity (cosine similarity)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # 3. Create graph from similarity matrix\n",
    "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "    # 4. Apply PageRank algorithm\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    # 5. Rank sentences and extract top ones\n",
    "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(original_sentences)), reverse=True)\n",
    "    \n",
    "    # Ensure we don't request more sentences than available\n",
    "    num_sentences = min(num_sentences, len(ranked_sentences))\n",
    "\n",
    "    # Get the top sentences\n",
    "    top_sentence_tuples = ranked_sentences[:num_sentences]\n",
    "    \n",
    "    # Sort them back to their original order for better readability\n",
    "    summary_sentences = sorted(top_sentence_tuples, key=lambda x: original_sentences.index(x[1]))\n",
    "\n",
    "    summary = \" \".join([s for _, s in summary_sentences])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo_md",
   "metadata": {},
   "source": [
    "## Section 5: Putting It All Together (Live Demo)\n",
    "\n",
    "Runs the summarization pipeline. If fetching fails, it tries AMP, a text extraction proxy, a local file, or manual paste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "demo_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed after 3 attempts: HTTP 403\n",
      "Request failed after 2 attempts: HTTP 403\n",
      "Request failed after 2 attempts: HTTP 403\n",
      "Fetched via r.jina.ai proxy: Title: á€áŸ’ášá»á˜á áŸŠá»á“ášáŠáŸ’á‹á…á·á“ á…á„áŸ‹á”ááŸ’áá¶á€áŸ‹á‘á»á“áœá·á“á·á™áŸ„á‚á›á¾á€á¶ášá€áŸ‚á…áŸ’á“áŸƒáŸáŸ’áœá¶á™á…á“áŸ’á‘á¸á“áŸ…á€á˜áŸ’á–á»á‡á¶\n",
      "\n",
      "==================================================\n",
      "ğŸ“° ARTICLE TITLE: Title: á€áŸ’ášá»á˜á áŸŠá»á“ášáŠáŸ’á‹á…á·á“ á…á„áŸ‹á”ááŸ’áá¶á€áŸ‹á‘á»á“áœá·á“á·á™áŸ„á‚á›á¾á€á¶ášá€áŸ‚á…áŸ’á“áŸƒáŸáŸ’áœá¶á™á…á“áŸ’á‘á¸á“áŸ…á€á˜áŸ’á–á»á‡á¶\n",
      "==================================================\n",
      "\n",
      "ğŸ“œ ORIGINAL TEXT (first 500 chars):\n",
      "Title: á€áŸ’ášá»á˜á áŸŠá»á“ášáŠáŸ’á‹á…á·á“ á…á„áŸ‹á”ááŸ’áá¶á€áŸ‹á‘á»á“áœá·á“á·á™áŸ„á‚á›á¾á€á¶ášá€áŸ‚á…áŸ’á“áŸƒáŸáŸ’áœá¶á™á…á“áŸ’á‘á¸á“áŸ…á€á˜áŸ’á–á»á‡á¶\n",
      "\n",
      "URL Source: http://thmeythmey.com/detail/151427\n",
      "\n",
      "Markdown Content:\n",
      "á€áŸ’ášá»á˜á áŸŠá»á“ášáŠáŸ’á‹á…á·á“ á…á„áŸ‹á”ááŸ’áá¶á€áŸ‹á‘á»á“áœá·á“á·á™áŸ„á‚á›á¾á€á¶ášá€áŸ‚á…áŸ’á“áŸƒáŸáŸ’áœá¶á™á…á“áŸ’á‘á¸á“áŸ…á€á˜áŸ’á–á»á‡á¶ \n",
      "\n",
      "===============\n",
      "\n",
      "*   [](https://www.facebook.com/ThmeyThmey/)\n",
      "*   [](https://twitter.com/thmeythmey)\n",
      "*   [](https://www.instagram.com/thmeythmeynews/)\n",
      "*   [](https://www.youtube.com/c/ThmeyThmeyOnlineNews)\n",
      "*   [](https://t.me/thmeythmeymedia)\n",
      "*   [](https://www.tiktok.com/@thmeythmey.co...\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "âœ¨ GENERATED SUMMARY:\n",
      "Title: á€áŸ’ášá»á˜á áŸŠá»á“ášáŠáŸ’á‹á…á·á“ á…á„áŸ‹á”ááŸ’áá¶á€áŸ‹á‘á»á“áœá·á“á·á™áŸ„á‚á›á¾á€á¶ášá€áŸ‚á…áŸ’á“áŸƒáŸáŸ’áœá¶á™á…á“áŸ’á‘á¸á“áŸ…á€á˜áŸ’á–á»á‡á¶\n",
      "\n",
      "URL Source: http://thmeythmey.com/detail/151427\n",
      "\n",
      "Markdown Content:\n",
      "á€áŸ’ášá»á˜á áŸŠá»á“ášáŠáŸ’á‹á…á·á“ á…á„áŸ‹á”ááŸ’áá¶á€áŸ‹á‘á»á“áœá·á“á·á™áŸ„á‚á›á¾á€á¶ášá€áŸ‚á…áŸ’á“áŸƒáŸáŸ’áœá¶á™á…á“áŸ’á‘á¸á“áŸ…á€á˜áŸ’á–á»á‡á¶ \n",
      "\n",
      "===============\n",
      "\n",
      "*   [](https://www.facebook.com/ThmeyThmey/)\n",
      "*   [](https://twitter.com/thmeythmey)\n",
      "*   [](https://www.instagram.com/thmeythmeynews/)\n",
      "*   [](https://www.youtube.com/c/ThmeyThmeyOnlineNews)\n",
      "*   [](https://t.me/thmeythmeymedia)\n",
      "*   [](https://www.tiktok.com/@thmeythmey.com)\n",
      "*   [](https://www.linkedin.com/company/thmeythmey/?originalSubdomain=kh)\n",
      "\n",
      "[! [Image 29: á€áŸ’ášá»á˜á áŸŠá»á“ášáŠáŸ’á‹á…á·á“ á…á„áŸ‹á”ááŸ’áá¶á€áŸ‹á‘á»á“áœá·á“á·á™áŸ„á‚á›á¾á€á¶ášá€áŸ‚á…áŸ’á“áŸƒáŸáŸ’áœá¶á™á…á“áŸ’á‘á¸á“áŸ…á€á˜áŸ’á–á»á‡á¶](https://image.thmeythmey.com/pictures/2025/09/12/image_68c3f3b4d5c24.jpg)\n",
      "\n",
      "* * *\n",
      "\n",
      "**á—áŸ’á“áŸ†á–áŸá‰áŸ– á”á“áŸ’á‘á¶á”áŸ‹á–á¸â€‹â€‹á’áŸ’áœá¾áŠáŸ†áá¾ášâ€‹á‘áŸáŸ’áŸá“á€á·á…áŸ’á…â€‹á“áŸ…â€‹á€á˜áŸ’á–á»á‡á¶ á€áŸ’ášá»á˜á áŸŠá»á“ Sinomach Hainan Development Co., Ltd. áŠáŸ‚á›á‡á¶á€áŸ’ášá»á˜á áŸŠá»á“ášáŠáŸ’á‹á…á·á“ á”á¶á“â€‹á…á¶á”áŸ‹â€‹á¢á¶ášá˜áŸ’á˜ááŸáœá·á“á·á™áŸ„á‚á“áŸ…á€á˜áŸ’á–á»á‡á¶áŸ” á€áŸ’ášá»á˜á áŸŠá»á“á“áŸáŸ‡ á‚áºá…á„áŸ‹â€‹á”ááŸ’áá¶á€áŸ‹á‘á»á“á›á¾â€‹áœá·áŸáŸá™á§áŸáŸ’áŸá¶á á€á˜áŸ’á˜áŸáŸ’áœá¶á™á…á“áŸ’á‘á¸áŸ” á”á¾áá¶á˜â€‹ááŸ†áá¶á„â€‹á€áŸ’ášá»á˜â€‹á áŸŠá»á“ á‚á˜áŸ’ášáŸ„á„áœá·á“á·á™áŸ„á‚á§áŸáŸ’áŸá¶á á€á˜áŸ’á˜áŸáŸ’áœá¶á™á…á“áŸ’á‘á¸á“áŸ…á€á˜áŸ’á–á»á‡á¶ á“á¹á„ášáŸ€á”á…áŸ†á‡á¶áŸ£áŠáŸ†áá¶á€áŸ‹á€á¶á› áŠáŸ‚á›áŠáŸ†áá¶á€áŸ‹á€á¶á›á‘á¸áŸ¡ á‚áŸ’ášáŸ„á„á€áŸ‚á…áŸ’á“áŸƒá‚áŸ’ášá¶á”áŸ‹áŸáŸ’áœá¶á™á…á“áŸ’á‘á¸á±áŸ’á™á”á¶á“áŸ£á˜áŸ‰áºá“ááŸ„á“á€áŸ’á“á»á„â€‹áŸ¡á†áŸ’á“á¶áŸ†, áŠáŸ†áá¶á€áŸ‹á€á¶á›á‘á¸áŸ¢ áŸ¢áŸ á˜áŸ‰áºá“ááŸ„á“á€áŸ’á“á»á„â€‹áŸ¡á†áŸ’á“á¶áŸ† á“á·á„áŠáŸ†áá¶á€áŸ‹á€á¶á›á‘á¸áŸ£ áŸ¥áŸ á˜áŸ‰áºá“ááŸ„á“á€áŸ’á“á»á„â€‹áŸ¡á†áŸ’á“á¶áŸ†áŸ”**\n",
      "\n",
      "**! [Image 32](https://image.thmeythmey.com/pictures/2025/09/12//image_34.jpg)\n",
      "\n",
      "_á›áŸ„á€ David Zou Zhun á”áŸ’ášá’á¶á“á•áŸ’á“áŸ‚á€á€áŸá·á€á˜áŸ’á˜á“áŸƒá€áŸ’ášá»á˜á áŸŠá»á“ Sinomach Hainan Development Co., Ltd_\n",
      "\n",
      "á‡á¶á˜á½á™á‚áŸ’á“á¶á“áŸáŸ‡ á›áŸ„á€ David Zou Zhun á”á¶á“â€‹á”á‰áŸ’á‡á¶á€áŸ‹á–á¸á‚á˜áŸ’ášáŸ„á„â€‹áœá·á“á·á™áŸ„á‚á§áŸáŸ’áŸá¶á á€á˜áŸ’á˜áŸáŸ’áœá¶á™á…á“áŸ’á‘á¸á“áŸ…á€á˜áŸ’á–á»á‡á¶áŠá¼á…áŸ’á“áŸáŸ‡áŸ–Â«á€áŸ’ášá»á˜á áŸŠá»á“á“á¹á„ášáŸ€á”á…áŸ†á‡á¶áŸ£áŠáŸ†áá¶á€áŸ‹á€á¶á› áŠáŸ‚á›áŠáŸ†áá¶á€áŸ‹á€á¶á›á‘á¸áŸ¡ á‚áŸ’ášáŸ„á„á€áŸ‚á…áŸ’á“áŸƒá‚áŸ’ášá¶á”áŸ‹áŸáŸ’áœá¶á™á…á“áŸ’á‘á¸á±áŸ’á™á”á¶á“ áŸ£á˜áŸ‰áºá“ááŸ„á“á€áŸ’á“á»á„áŸ¡á†áŸ’á“á¶áŸ†, áŠáŸ†áá¶á€áŸ‹á€á¶á›á‘á¸áŸ¢ áŸ¢áŸ á˜áŸ‰áºá“ááŸ„á“á€áŸ’á“á»á„áŸ¡á†áŸ’á“á¶áŸ† á“á·á„áŠáŸ†áá¶á€áŸ‹á€á¶á›á‘á¸áŸ£ áŸ¥áŸ á˜áŸ‰áºá“ááŸ„á“á€áŸ’á“á»á„áŸ¡á†áŸ’á“á¶áŸ†Â»áŸ”\n",
      "\n",
      "áŸá¼á˜â€‹á”á‰áŸ’á‡á¶á€áŸ‹áá¶ á“áŸ…á€áŸ’á“á»á„á†á˜á¶áŸá‘á¸áŸ¡ á†áŸ’á“á¶áŸ†áŸ¢áŸ áŸ¢áŸ¥ á€á˜áŸ’á–á»á‡á¶á”á¶á“á“á¶áŸ†á…áŸá‰á‚áŸ’ášá¶á”áŸ‹áŸáŸ’áœá¶á™á…á“áŸ’á‘á¸á”áŸ’ášá˜á¶á áŸ¥áŸ§ áŸ¨áŸ áŸ£ áŸ¢áŸ¥áŸ¥ááŸ„á“ á‘áŸ…á€á¶á“áŸ‹á‘á¸á•áŸ’áŸá¶ášá¢á“áŸ’áášá‡á¶áá· á€á¾á“á¡á¾á„ áŸ¥,áŸ©áŸ¨á—á¶á‚ášá™ á’áŸ€á”á“á¹á„ášá™áŸˆá–áŸá›áŠá¼á…á‚áŸ’á“á¶á†áŸ’á“á¶áŸ†áŸ¢áŸ áŸ¢áŸ¤áŸ” á“áŸáŸ‡á”á¾á™áŸ„á„áá¶á˜ášá”á¶á™á€á¶ášááŸášá”áŸáŸ‹á¢á‚áŸ’á‚á“á¶á™á€áŠáŸ’á‹á¶á“á€áŸá·á€á˜áŸ’á˜á“áŸƒá€áŸ’ášáŸá½á„á€áŸá·á€á˜áŸ’á˜ ášá»á€áŸ’áá¶á”áŸ’ášá˜á¶á‰áŸ‹ á“á·á„á“áŸáŸá¶á‘áŸ•\n",
      "\n",
      "á¢ááŸ’áá”á‘á‘á¶á€áŸ‹á‘á„\n",
      "\n",
      "[á¥ááŸ’áŒá¶ á˜á¶á“á‚á˜áŸ’ášáŸ„á„á”á„áŸ’á€á¾á“á€á¶ášá“á¶áŸ†á…á¼á›á‚áŸ’ášá¶á”áŸ‹áŸáŸ’áœá¶á™á…á“áŸ’á‘á¸áŸáŸ’ášáŸáŸ‹á”á“áŸ’ááŸ‚á˜á–á¸á€á˜áŸ’á–á»á‡á¶](https://thmeythmey.com/detail/151263)\n",
      "\n",
      "[á€á˜áŸ’á–á»á‡á¶á”á¶á“á“á¶áŸ†á…áŸá‰á‚áŸ’ášá¶á”áŸ‹áŸáŸ’áœá¶á™á…á“áŸ’á‘á¸á€áŸ‚á…áŸ’á“áŸƒ áŸ¡áŸ¤ááŸ„á“ á‘áŸ…á”áŸ’ášá‘áŸáŸá¢áŸŠá¸áá¶á›á¸á‡á¶á›á¾á€áŠáŸ†á”á¼á„](https://thmeythmey.com/detail/151130)\n",
      "\n",
      "[á‚áŸ’ášá¶á”áŸ‹áŸáŸ’áœá¶á™á…á“áŸ’á‘á¸á€á˜áŸ’á–á»á‡á¶ á€áŸ†á–á»á„áŠá¶á€áŸ‹á›á€áŸ‹á€áŸ’á“á»á„ Â«Family MartÂ» á“áŸ…á€áŸ’á“á»á„á”áŸ’ášá‘áŸáŸá‡á”áŸ‰á»á“](https://thmeythmey.com/detail/151123)\n",
      "\n",
      "* * *\n",
      "\n",
      "á¢áŸ’á“á€áŸášáŸáŸášá¢ááŸ’áá”á‘\n",
      "\n",
      "!\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "\n",
    "# URL of a real news article\n",
    "# You can replace this with another article from vodkhmer.news\n",
    "ARTICLE_URL = \"https://thmeythmey.com/detail/151427\"\n",
    "\n",
    "# 1. Scrape the article\n",
    "title, article_text = scrape_vod_article(ARTICLE_URL)\n",
    "\n",
    "if article_text:\n",
    "    # 2. Preprocess the text\n",
    "    original_sentences, processed_sentences = preprocess_khmer_text(article_text)\n",
    "    \n",
    "    # 3. Generate the summary\n",
    "    summary = summarize_textrank(original_sentences, processed_sentences, num_sentences=3)\n",
    "    \n",
    "    # --- Display Results ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"ğŸ“° ARTICLE TITLE: {title}\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    print(\"ğŸ“œ ORIGINAL TEXT (first 500 chars):\")\n",
    "    print(article_text[:500] + \"...\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    print(\"âœ¨ GENERATED SUMMARY:\")\n",
    "    print(summary)\n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_md",
   "metadata": {},
   "source": [
    "## Section 6: Evaluation (ROUGE Score)\n",
    "\n",
    "Compare the generated summary against a baseline (first 3 sentences). For Khmer, stemming is disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eval_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š EVALUATION (vs. first 3 sentences):\n",
      "  - ROUGE-1 (Overlap of unigrams): 0.6984\n",
      "  - ROUGE-2 (Overlap of bigrams): 0.6203\n",
      "  - ROUGE-L (Longest common subsequence): 0.6667\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "if article_text:\n",
    "    # Create a reference summary (baseline: first 3 sentences)\n",
    "    reference_summary = \" \".join(original_sentences[:3])\n",
    "\n",
    "    # Initialize the ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Calculate scores\n",
    "    scores = scorer.score(reference_summary, summary)\n",
    "\n",
    "    print(\"\\nğŸ“Š EVALUATION (vs. first 3 sentences):\")\n",
    "    print(f\"  - ROUGE-1 (Overlap of unigrams): {scores['rouge1'].fmeasure:.4f}\")\n",
    "    print(f\"  - ROUGE-2 (Overlap of bigrams): {scores['rouge2'].fmeasure:.4f}\")\n",
    "    print(f\"  - ROUGE-L (Longest common subsequence): {scores['rougeL'].fmeasure:.4f}\")\n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
