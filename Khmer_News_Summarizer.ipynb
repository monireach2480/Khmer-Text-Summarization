{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_md",
   "metadata": {},
   "source": [
    "# ITM 454: Khmer News Summarizer - Final Project\n",
    "\n",
    "This notebook implements:\n",
    "- A Khmer extractive text summarizer (TextRank).\n",
    "- A Machine Learning text classification demo using NLTK (Naive Bayes).\n",
    "\n",
    "Pipeline:\n",
    "1.  Data Collection (scraping with robust fallbacks)\n",
    "2.  Preprocessing (Khmer-aware tokenization and normalization)\n",
    "3.  Summarization (TextRank)\n",
    "4.  Evaluation (ROUGE)\n",
    "5.  ML Implementation (NLTK Naive Bayes text classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "setup_imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup and Imports\n",
    "# Install necessary libraries if you haven't already using requirements.txt\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from khmernltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# NLTK's sentence tokenizer is generally language-agnostic and works well for sentence boundaries.\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "tokenizers_util",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_vod_article(url):\n",
    "    \"\"\"\n",
    "    Scrapes the title and article text from a VOD Khmer news URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: Failed to fetch URL with status code {response.status_code}\")\n",
    "            return None, None\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the article title\n",
    "        title = soup.find('h1', class_='entry-title')\n",
    "        if not title:\n",
    "            print(\"Error: Could not find the article title.\")\n",
    "            return None, None\n",
    "        title_text = title.get_text().strip()\n",
    "\n",
    "        # Find the main article content\n",
    "        content_div = soup.find('div', class_='entry-content')\n",
    "        if not content_div:\n",
    "            print(\"Error: Could not find the article content.\")\n",
    "            return None, None\n",
    "            \n",
    "        paragraphs = content_div.find_all('p')\n",
    "        article_text = ' '.join([p.get_text().strip() for p in paragraphs])\n",
    "        \n",
    "        print(f\" Scraping successful for: {title_text}\")\n",
    "        return title_text, article_text\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred during scraping: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scraper_md",
   "metadata": {},
   "source": [
    "## Section 2: Data Collection (Web Scraper)\n",
    "\n",
    "Robust scraping with headers, retries, AMP fallback and r.jina.ai proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "scraper_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _request_with_retries(url, headers=None, timeout=10, retries=3, backoff=1.3):\n",
    "    headers = headers or {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'km-KH,km;q=0.9,en;q=0.8'\n",
    "    }\n",
    "    last_exc = None\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "            if resp.status_code == 200:\n",
    "                return resp\n",
    "            else:\n",
    "                last_exc = Exception(f'HTTP {resp.status_code}')\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "        time.sleep(backoff ** attempt)\n",
    "    if last_exc:\n",
    "        print(f'Request failed after {retries} attempts: {last_exc}')\n",
    "    return None\n",
    "\n",
    "def scrape_vod_article(url):\n",
    "    \"\"\"\n",
    "    Scrapes the title and article text from a Khmer news URL (with fallbacks).\n",
    "    Returns: (title, text) or (None, None)\n",
    "    \"\"\"\n",
    "    # 1) Try direct fetch\n",
    "    response = _request_with_retries(url, timeout=10, retries=3, backoff=1.3)\n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title_el = soup.find('h1', class_='entry-title') or soup.find('h1')\n",
    "        title_text = title_el.get_text(strip=True) if title_el else None\n",
    "        content_div = soup.find('div', class_='entry-content')\n",
    "        paragraphs = []\n",
    "        if content_div:\n",
    "            paragraphs = content_div.find_all('p')\n",
    "        if not paragraphs:\n",
    "            paragraphs = soup.find_all('p')\n",
    "        article_text = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "        article_text = re.sub(r'\\s+', ' ', article_text).strip()\n",
    "        if article_text:\n",
    "            print(f\"Scraping successful for: {title_text or url}\")\n",
    "            return title_text, article_text\n",
    "\n",
    "    # 2) Try AMP page variants\n",
    "    amp_candidates = [url.rstrip('/') + '/amp/', url.rstrip('/') + '/amp']\n",
    "    for amp_url in amp_candidates:\n",
    "        response = _request_with_retries(amp_url, timeout=10, retries=2, backoff=1.2)\n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title_el = soup.find('h1')\n",
    "            title_text = title_el.get_text(strip=True) if title_el else None\n",
    "            paragraphs = soup.find_all('p')\n",
    "            article_text = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "            article_text = re.sub(r'\\s+', ' ', article_text).strip()\n",
    "            if article_text:\n",
    "                print(f\"Scraped AMP page for: {title_text or url}\")\n",
    "                return title_text, article_text\n",
    "\n",
    "    # 3) Try r.jina.ai content extraction proxy\n",
    "    try:\n",
    "        scheme_split = url.split('://', 1)\n",
    "        if len(scheme_split) == 2:\n",
    "            rjina_url = f\"https://r.jina.ai/http://{scheme_split[1]}\"\n",
    "        else:\n",
    "            rjina_url = f\"https://r.jina.ai/http://{url}\"\n",
    "        r = _request_with_retries(rjina_url, timeout=10, retries=2, backoff=1.2)\n",
    "        if r and r.status_code == 200:\n",
    "            text = r.text.strip()\n",
    "            if text:\n",
    "                lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "                title_text = lines[0] if lines else None\n",
    "                print(f\"Fetched via r.jina.ai proxy: {title_text or url}\")\n",
    "                return title_text, text\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print('Error: Could not fetch article content from the URL or fallbacks.')\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocess_md",
   "metadata": {},
   "source": [
    "## Section 3: Text Preprocessing Pipeline\n",
    "\n",
    "Cleans and prepares Khmer text for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "preprocess_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom list of Khmer stopwords. A more comprehensive list would improve results.\n",
    "KHMER_STOPWORDS = [\n",
    "    'និង', 'នៃ', 'ក្នុង', 'ជា', 'នៅ', 'បាន', 'ថា', 'ដោយ', 'ដែរ', 'さんも',\n",
    "    'ទៅ', 'ឲ្យ', 'ពី', 'មួយ', 'ៗ', '។', 'ៗ', 'ដែល', 'មាន', 'การ', 'นี้',\n",
    "    'លោក', 'អ្នក', 'ខ្ញុំ', 'គេ', 'យើង', 'វា', 'គាត់', 'นั้น', 'នេះ', 'ទេ', 'ដែរ'\n",
    "]\n",
    "\n",
    "def preprocess_khmer_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses Khmer text for summarization.\n",
    "    \"\"\"\n",
    "    # 1. Sentence Segmentation\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # 2. Clean and tokenize each sentence\n",
    "    processed_sentences = []\n",
    "    for sent in sentences:\n",
    "        # Lowercase and remove non-Khmer characters/punctuation (keeping spaces)\n",
    "        sent = re.sub(r'[a-zA-Z0-9\\.,!?\\(\\)\\[\\]\\{\\}\"\\':;]', '', sent)\n",
    "        sent = sent.strip()\n",
    "        \n",
    "        # 3. Word Tokenization using khmernltk\n",
    "        tokens = word_tokenize(sent)\n",
    "        \n",
    "        # 4. Stopword Removal\n",
    "        filtered_tokens = [word for word in tokens if word not in KHMER_STOPWORDS and len(word) > 1]\n",
    "        \n",
    "        if filtered_tokens:\n",
    "            processed_sentences.append(\" \".join(filtered_tokens))\n",
    "            \n",
    "    return sentences, processed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summarize_md",
   "metadata": {},
   "source": [
    "## Section 4: The Summarization Model (TextRank)\n",
    "\n",
    "Represent each sentence as a TF-IDF vector, compute similarities, apply PageRank, and select top sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "summarize_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_textrank(original_sentences, processed_sentences, num_sentences=3):\n",
    "    \"\"\"\n",
    "    Generates a summary using the TextRank algorithm.\n",
    "    \"\"\"\n",
    "    if not processed_sentences:\n",
    "        return \"Could not generate summary. No content after preprocessing.\"\n",
    "\n",
    "    # 1. Vectorize sentences using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n",
    "\n",
    "    # 2. Calculate sentence similarity (cosine similarity)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # 3. Create graph from similarity matrix\n",
    "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "    # 4. Apply PageRank algorithm\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    # 5. Rank sentences and extract top ones\n",
    "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(original_sentences)), reverse=True)\n",
    "    \n",
    "    # Ensure we don't request more sentences than available\n",
    "    num_sentences = min(num_sentences, len(ranked_sentences))\n",
    "\n",
    "    # Get the top sentences\n",
    "    top_sentence_tuples = ranked_sentences[:num_sentences]\n",
    "    \n",
    "    # Sort them back to their original order for better readability\n",
    "    summary_sentences = sorted(top_sentence_tuples, key=lambda x: original_sentences.index(x[1]))\n",
    "\n",
    "    summary = \" \".join([s for _, s in summary_sentences])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo_md",
   "metadata": {},
   "source": [
    "## Section 5: Putting It All Together (Live Demo)\n",
    "\n",
    "Runs the summarization pipeline. If fetching fails, it tries AMP, a text extraction proxy, a local file, or manual paste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "demo_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed after 3 attempts: HTTP 403\n",
      "Request failed after 2 attempts: HTTP 403\n",
      "Request failed after 2 attempts: HTTP 403\n",
      "Fetched via r.jina.ai proxy: Title: ក្រុមហ៊ុនរដ្ឋចិន ចង់បណ្តាក់ទុនវិនិយោគលើការកែច្នៃស្វាយចន្ទីនៅកម្ពុជា\n",
      "\n",
      "==================================================\n",
      "📰 ARTICLE TITLE: Title: ក្រុមហ៊ុនរដ្ឋចិន ចង់បណ្តាក់ទុនវិនិយោគលើការកែច្នៃស្វាយចន្ទីនៅកម្ពុជា\n",
      "==================================================\n",
      "\n",
      "📜 ORIGINAL TEXT (first 500 chars):\n",
      "Title: ក្រុមហ៊ុនរដ្ឋចិន ចង់បណ្តាក់ទុនវិនិយោគលើការកែច្នៃស្វាយចន្ទីនៅកម្ពុជា\n",
      "\n",
      "URL Source: http://thmeythmey.com/detail/151427\n",
      "\n",
      "Markdown Content:\n",
      "ក្រុមហ៊ុនរដ្ឋចិន ចង់បណ្តាក់ទុនវិនិយោគលើការកែច្នៃស្វាយចន្ទីនៅកម្ពុជា \n",
      "\n",
      "===============\n",
      "\n",
      "*   [](https://www.facebook.com/ThmeyThmey/)\n",
      "*   [](https://twitter.com/thmeythmey)\n",
      "*   [](https://www.instagram.com/thmeythmeynews/)\n",
      "*   [](https://www.youtube.com/c/ThmeyThmeyOnlineNews)\n",
      "*   [](https://t.me/thmeythmeymedia)\n",
      "*   [](https://www.tiktok.com/@thmeythmey.co...\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "✨ GENERATED SUMMARY:\n",
      "Title: ក្រុមហ៊ុនរដ្ឋចិន ចង់បណ្តាក់ទុនវិនិយោគលើការកែច្នៃស្វាយចន្ទីនៅកម្ពុជា\n",
      "\n",
      "URL Source: http://thmeythmey.com/detail/151427\n",
      "\n",
      "Markdown Content:\n",
      "ក្រុមហ៊ុនរដ្ឋចិន ចង់បណ្តាក់ទុនវិនិយោគលើការកែច្នៃស្វាយចន្ទីនៅកម្ពុជា \n",
      "\n",
      "===============\n",
      "\n",
      "*   [](https://www.facebook.com/ThmeyThmey/)\n",
      "*   [](https://twitter.com/thmeythmey)\n",
      "*   [](https://www.instagram.com/thmeythmeynews/)\n",
      "*   [](https://www.youtube.com/c/ThmeyThmeyOnlineNews)\n",
      "*   [](https://t.me/thmeythmeymedia)\n",
      "*   [](https://www.tiktok.com/@thmeythmey.com)\n",
      "*   [](https://www.linkedin.com/company/thmeythmey/?originalSubdomain=kh)\n",
      "\n",
      "[! [Image 29: ក្រុមហ៊ុនរដ្ឋចិន ចង់បណ្តាក់ទុនវិនិយោគលើការកែច្នៃស្វាយចន្ទីនៅកម្ពុជា](https://image.thmeythmey.com/pictures/2025/09/12/image_68c3f3b4d5c24.jpg)\n",
      "\n",
      "* * *\n",
      "\n",
      "**ភ្នំពេញ៖ បន្ទាប់ពី​​ធ្វើដំណើរ​ទស្សនកិច្ច​នៅ​កម្ពុជា ក្រុមហ៊ុន Sinomach Hainan Development Co., Ltd. ដែលជាក្រុមហ៊ុនរដ្ឋចិន បាន​ចាប់​អារម្មណ៍វិនិយោគនៅកម្ពុជា។ ក្រុមហ៊ុននេះ គឺចង់​បណ្តាក់ទុនលើ​វិស័យឧស្សាហកម្មស្វាយចន្ទី។ បើតាម​តំណាង​ក្រុម​ហ៊ុន គម្រោងវិនិយោគឧស្សាហកម្មស្វាយចន្ទីនៅកម្ពុជា នឹងរៀបចំជា៣ដំណាក់កាល ដែលដំណាក់កាលទី១ គ្រោងកែច្នៃគ្រាប់ស្វាយចន្ទីឱ្យបាន៣ម៉ឺនតោនក្នុង​១ឆ្នាំ, ដំណាក់កាលទី២ ២០ម៉ឺនតោនក្នុង​១ឆ្នាំ និងដំណាក់កាលទី៣ ៥០ម៉ឺនតោនក្នុង​១ឆ្នាំ។**\n",
      "\n",
      "**! [Image 32](https://image.thmeythmey.com/pictures/2025/09/12//image_34.jpg)\n",
      "\n",
      "_លោក David Zou Zhun ប្រធានផ្នែកកសិកម្មនៃក្រុមហ៊ុន Sinomach Hainan Development Co., Ltd_\n",
      "\n",
      "ជាមួយគ្នានេះ លោក David Zou Zhun បាន​បញ្ជាក់ពីគម្រោង​វិនិយោគឧស្សាហកម្មស្វាយចន្ទីនៅកម្ពុជាដូច្នេះ៖«ក្រុមហ៊ុននឹងរៀបចំជា៣ដំណាក់កាល ដែលដំណាក់កាលទី១ គ្រោងកែច្នៃគ្រាប់ស្វាយចន្ទីឱ្យបាន ៣ម៉ឺនតោនក្នុង១ឆ្នាំ, ដំណាក់កាលទី២ ២០ម៉ឺនតោនក្នុង១ឆ្នាំ និងដំណាក់កាលទី៣ ៥០ម៉ឺនតោនក្នុង១ឆ្នាំ»។\n",
      "\n",
      "សូម​បញ្ជាក់ថា នៅក្នុងឆមាសទី១ ឆ្នាំ២០២៥ កម្ពុជាបាននាំចេញគ្រាប់ស្វាយចន្ទីប្រមាណ ៥៧ ៨០៣ ២៥៥តោន ទៅកាន់ទីផ្សារអន្តរជាតិ កើនឡើង ៥,៩៨ភាគរយ ធៀបនឹងរយៈពេលដូចគ្នាឆ្នាំ២០២៤។ នេះបើយោងតាមរបាយការណ៍របស់អគ្គនាយកដ្ឋានកសិកម្មនៃក្រសួងកសិកម្ម រុក្ខាប្រមាញ់ និងនេសាទ៕\n",
      "\n",
      "អត្ថបទទាក់ទង\n",
      "\n",
      "[ឥណ្ឌា មានគម្រោងបង្កើនការនាំចូលគ្រាប់ស្វាយចន្ទីស្រស់បន្ថែមពីកម្ពុជា](https://thmeythmey.com/detail/151263)\n",
      "\n",
      "[កម្ពុជាបាននាំចេញគ្រាប់ស្វាយចន្ទីកែច្នៃ ១៤តោន ទៅប្រទេសអ៊ីតាលីជាលើកដំបូង](https://thmeythmey.com/detail/151130)\n",
      "\n",
      "[គ្រាប់ស្វាយចន្ទីកម្ពុជា កំពុងដាក់លក់ក្នុង «Family Mart» នៅក្នុងប្រទេសជប៉ុន](https://thmeythmey.com/detail/151123)\n",
      "\n",
      "* * *\n",
      "\n",
      "អ្នកសរសេរអត្ថបទ\n",
      "\n",
      "!\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "\n",
    "# URL of a real news article\n",
    "# You can replace this with another article from vodkhmer.news\n",
    "ARTICLE_URL = \"https://thmeythmey.com/detail/151427\"\n",
    "\n",
    "# 1. Scrape the article\n",
    "title, article_text = scrape_vod_article(ARTICLE_URL)\n",
    "\n",
    "if article_text:\n",
    "    # 2. Preprocess the text\n",
    "    original_sentences, processed_sentences = preprocess_khmer_text(article_text)\n",
    "    \n",
    "    # 3. Generate the summary\n",
    "    summary = summarize_textrank(original_sentences, processed_sentences, num_sentences=3)\n",
    "    \n",
    "    # --- Display Results ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"📰 ARTICLE TITLE: {title}\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    print(\"📜 ORIGINAL TEXT (first 500 chars):\")\n",
    "    print(article_text[:500] + \"...\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    print(\"✨ GENERATED SUMMARY:\")\n",
    "    print(summary)\n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_md",
   "metadata": {},
   "source": [
    "## Section 6: Evaluation (ROUGE Score)\n",
    "\n",
    "Compare the generated summary against a baseline (first 3 sentences). For Khmer, stemming is disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eval_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 EVALUATION (vs. first 3 sentences):\n",
      "  - ROUGE-1 (Overlap of unigrams): 0.6984\n",
      "  - ROUGE-2 (Overlap of bigrams): 0.6203\n",
      "  - ROUGE-L (Longest common subsequence): 0.6667\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "if article_text:\n",
    "    # Create a reference summary (baseline: first 3 sentences)\n",
    "    reference_summary = \" \".join(original_sentences[:3])\n",
    "\n",
    "    # Initialize the ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Calculate scores\n",
    "    scores = scorer.score(reference_summary, summary)\n",
    "\n",
    "    print(\"\\n📊 EVALUATION (vs. first 3 sentences):\")\n",
    "    print(f\"  - ROUGE-1 (Overlap of unigrams): {scores['rouge1'].fmeasure:.4f}\")\n",
    "    print(f\"  - ROUGE-2 (Overlap of bigrams): {scores['rouge2'].fmeasure:.4f}\")\n",
    "    print(f\"  - ROUGE-L (Longest common subsequence): {scores['rougeL'].fmeasure:.4f}\")\n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
