{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_md",
   "metadata": {},
   "source": [
    "# ITM 454: Khmer News Summarizer - Final Project\n",
    "\n",
    "This notebook implements:\n",
    "- A Khmer extractive text summarizer (TextRank).\n",
    "- A Machine Learning text classification demo using NLTK (Naive Bayes).\n",
    "\n",
    "Pipeline:\n",
    "1.  Data Collection (scraping with robust fallbacks)\n",
    "2.  Preprocessing (Khmer-aware tokenization and normalization)\n",
    "3.  Summarization (TextRank)\n",
    "4.  Evaluation (ROUGE)\n",
    "5.  ML Implementation (NLTK Naive Bayes text classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "setup_imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Core Libraries ---\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# --- NLP Libraries ---\n",
    "import nltk\n",
    "from khmernltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# --- Visualization (Optional but Recommended) ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tokenizers_util",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded and processed 1048 articles from train.json\n"
     ]
    }
   ],
   "source": [
    "def load_data_from_json(file_path='train.json'):\n",
    "    \"\"\"\n",
    "    Loads and transforms data from the specified JSON file.\n",
    "    Maps 'text' to 'title' and 'detail[0]' to 'full_text'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        dataset = []\n",
    "        for item in json_data:\n",
    "            if 'detail' in item and item['detail']:\n",
    "                dataset.append({\n",
    "                    'title': item.get('text', 'No Title'),\n",
    "                    'full_text': item['detail'][0]\n",
    "                })\n",
    "        \n",
    "        print(f\"✅ Successfully loaded and processed {len(dataset)} articles from {file_path}\")\n",
    "        return dataset\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: '{file_path}' not found. Please ensure it is in the same directory.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Load the Articles ---\n",
    "article_dataset = load_data_from_json('train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2672ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded and processed 1048 articles from train.json\n"
     ]
    }
   ],
   "source": [
    "def load_data_from_json(file_path='train.json'):\n",
    "    \"\"\"\n",
    "    Loads and transforms data from the specified JSON file.\n",
    "    Maps 'text' to 'title' and 'detail[0]' to 'full_text'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        dataset = []\n",
    "        for item in json_data:\n",
    "            if 'detail' in item and item['detail']:\n",
    "                dataset.append({\n",
    "                    'title': item.get('text', 'No Title'),\n",
    "                    'full_text': item['detail'][0]\n",
    "                })\n",
    "        \n",
    "        print(f\"✅ Successfully loaded and processed {len(dataset)} articles from {file_path}\")\n",
    "        return dataset\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: '{file_path}' not found. Please ensure it is in the same directory.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Load the Articles ---\n",
    "article_dataset = load_data_from_json('train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eb65ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "KHMER_STOPWORDS = [\n",
    "    'និង', 'នៃ', 'ក្នុង', 'ជា', 'នៅ', 'បាន', 'ថា', 'ដោយ', 'ដែរ', 'ទៅ', 'ឲ្យ', \n",
    "    'ពី', 'មួយ', 'ៗ', '។', 'ដែល', 'មាន', 'លោក', 'អ្នក', 'ខ្ញុំ', 'គេ', 'យើង', \n",
    "    'វា', 'គាត់', 'นั้น', 'នេះ', 'ទេ'\n",
    "]\n",
    "\n",
    "def preprocess_khmer_text(text):\n",
    "    \"\"\"\n",
    "    Implements the full preprocessing pipeline for Khmer text.\n",
    "    Returns the original sentences and the cleaned, tokenized sentences.\n",
    "    \"\"\"\n",
    "    # 1. Sentence Segmentation (using NLTK)\n",
    "    original_sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    processed_sentences = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sent in original_sentences:\n",
    "        # 3a. Punctuation Removal\n",
    "        sent_clean = re.sub(r'[a-zA-Z0-9\\.,!?\\(\\)\\[\\]\\{\\}\"\\':;]', '', sent)\n",
    "        sent_clean = sent_clean.strip()\n",
    "        \n",
    "        # 2. Word Tokenization\n",
    "        tokens = word_tokenize(sent_clean)\n",
    "        \n",
    "        # 3b. Stopword Removal\n",
    "        filtered_tokens = [\n",
    "            word for word in tokens \n",
    "            if word not in KHMER_STOPWORDS and len(word) > 1\n",
    "        ]\n",
    "        \n",
    "        if filtered_tokens:\n",
    "            processed_sentences.append(\" \".join(filtered_tokens))\n",
    "            all_tokens.extend(filtered_tokens)\n",
    "            \n",
    "    return original_sentences, processed_sentences, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "011d48db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_and_plot_word_frequency(all_tokens, num_words=15):\n",
    "    \"\"\"\n",
    "    Uses NLTK's FreqDist to analyze and plot word frequency.\n",
    "    \"\"\"\n",
    "    if not all_tokens:\n",
    "        print(\"  -> No tokens to analyze.\")\n",
    "        return\n",
    "\n",
    "    # Use NLTK's Frequency Distribution\n",
    "    freq_dist = nltk.FreqDist(all_tokens)\n",
    "    \n",
    "    print(f\"\\n📊 Top {num_words} Most Common Words:\")\n",
    "    for word, freq in freq_dist.most_common(num_words):\n",
    "        print(f\"  - {word}: {freq}\")\n",
    "        \n",
    "    # --- Visualization ---\n",
    "    # Note: You might need to install a Khmer font for your system\n",
    "    # for the plot labels to render correctly.\n",
    "    try:\n",
    "        # For Khmer fonts in Matplotlib\n",
    "        font_path = '/usr/share/fonts/truetype/khmeros/KhmerOS.ttf' # Example for Linux\n",
    "        khmer_font = FontProperties(fname=font_path)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        freq_dist.plot(num_words, cumulative=False)\n",
    "        plt.xlabel(\"Words\", fontproperties=khmer_font)\n",
    "        plt.ylabel(\"Frequency\", fontproperties=khmer_font)\n",
    "        plt.title(f\"Top {num_words} Word Frequencies\", fontproperties=khmer_font)\n",
    "        plt.xticks(fontproperties=khmer_font, rotation=45)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[Plotting Skipped] Could not generate plot. Matplotlib or font setup might be needed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba2665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
